{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm9yyq2Q97hh"},"outputs":[],"source":["#IMPORT LIBRARIES\n","import numpy as np\n","import pandas as pd\n","import fnmatch\n","import math\n","import time\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","%matplotlib inline\n","#import missingno as msno\n","\n","from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GroupShuffleSplit\n","from sklearn.model_selection import GroupKFold, GridSearchCV\n","from sklearn import linear_model, metrics\n","from sklearn.svm import SVR, LinearSVR\n","from xgboost import XGBRegressor\n","from scipy.stats import uniform, randint\n","\n","from sklearn.linear_model import LassoCV\n","from sklearn.linear_model import LassoLarsCV\n","\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.compose import ColumnTransformer, make_column_transformer"]},{"cell_type":"markdown","metadata":{"id":"UQWhQMoC97hj"},"source":["# DKT FS Loop with Lasso-LLars"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZTGrqWos97hk"},"outputs":[],"source":["%%capture --no-stderr capture\n","\n","for j in range(3,5): # MODIFY to (1,5) for all 4 datasets\n","    for z in range(1,4): # MODIFY to (1,4) for all 3 variables\n","        for q in range(1,21):\n","            data = j\n","            variable = z\n","            estimator = 1\n","            imaging = q\n","            #####################\n","            # DATASET SELECTION #\n","            #####################\n","\n","            if data == 1 and variable == 1:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 2:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 3:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 2 and variable == 1:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 2:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 3:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 3 and variable == 1:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 2:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 3:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 4 and variable == 1:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 2:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 3:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_hub.csv\", index_col=0)\n","            else:\n","                print(\"Data and Variable Not Selected\")\n","\n","            #######################\n","            # IMPORT IMAGING DATA #\n","            #######################\n","\n","            if imaging == 1:\n","                df_i = pd.read_csv('1.DKTarea_ICVresid_hub.csv'); i_filename = 'DKTarea_ICVresid_hub.csv' #1\n","            elif imaging == 2:\n","                df_i = pd.read_csv('1.DKTarea_SA_hub.csv'); i_filename = 'DKTarea_SA_hub.csv' #2\n","            elif imaging == 3:\n","                df_i = pd.read_csv('1.DKTarea_SAresid_hub.csv'); i_filename = 'DKTarea_SAresid_hub.csv' #3\n","            elif imaging == 4:\n","                df_i = pd.read_csv('1.DKTarea_asym_hub.csv'); i_filename = 'DKTarea_asym_hub.csv'\n","            elif imaging == 5:\n","                df_i = pd.read_csv('1.DKTarea_hub.csv'); i_filename = 'DKTarea_hub.csv'\n","            elif imaging == 6:\n","                df_i = pd.read_csv('1.DKTaseg_ICVresid_hub.csv'); i_filename = 'DKTaseg_ICVresid_hub.csv'\n","            elif imaging == 7:\n","                df_i = pd.read_csv('1.DKTaseg_hub.csv'); i_filename = 'DKTaseg_hub.csv' #4\n","            elif imaging == 8:\n","                df_i = pd.read_csv('1.DKTlgi_ICVresid_hub.csv'); i_filename = 'DKTlgi_ICVresid_hub.csv'\n","            elif imaging == 9:\n","                df_i = pd.read_csv('1.DKTlgi_SAresid_hub.csv'); i_filename = 'DKTlgi_SAresid_hub.csv'\n","            elif imaging == 10:\n","                df_i = pd.read_csv('1.DKTlgi_asym_hub.csv'); i_filename = 'DKTlgi_asym_hub.csv'\n","            elif imaging == 11:\n","                df_i = pd.read_csv('1.DKTlgi_hub.csv'); i_filename = 'DKTlgi_hub.csv'\n","            elif imaging == 12:\n","                df_i = pd.read_csv('1.DKTthick_ICVresid_hub.csv'); i_filename = 'DKTthick_ICVresid_hub.csv' #5\n","            elif imaging == 13:\n","                df_i = pd.read_csv('1.DKTthick_MT_hub.csv'); i_filename = 'DKTthick_MT_hub.csv' #6\n","            elif imaging == 14:\n","                df_i = pd.read_csv('1.DKTthick_MTresid_hub.csv'); i_filename = 'DKTthick_MTresid_hub.csv' #7\n","            elif imaging == 15:\n","                df_i = pd.read_csv('1.DKTthick_asym_hub.csv'); i_filename = 'DKTthick_asym_hub.csv'\n","            elif imaging == 16:\n","                df_i = pd.read_csv('1.DKTthick_hub.csv'); i_filename = 'DKTthick_hub.csv'\n","            elif imaging == 17:\n","                df_i = pd.read_csv('1.DKTvol_ICVresid_hub.csv'); i_filename = 'DKTvol_ICVresid_hub.csv'\n","            elif imaging == 18:\n","                df_i = pd.read_csv('1.DKTvol_SAresid_hub.csv'); i_filename = 'DKTvol_SAresid_hub.csv'\n","            elif imaging == 19:\n","                df_i = pd.read_csv('1.DKTvol_asym_hub.csv'); i_filename = 'DKTvol_asym_hub.csv'\n","            elif imaging == 20:\n","                df_i = pd.read_csv('1.DKTvol_hub.csv'); i_filename = 'DKTvol_hub.csv'\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","\n","            ################################\n","            # COMBINE BEHAVIORAL & IMAGING #\n","            ################################\n","\n","            if data == 3 or data == 4:\n","                df_with_i = df.merge(df_i, on='scanid1')\n","                df_with_i = df_with_i.dropna()\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df.rename(columns={'Unnamed: 0_x':'Unnamed: 0'}, inplace=True)\n","            #    df_with_i.drop(df_with_i.columns[13], axis=1, inplace=True)\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df_with_i.reset_index(drop=True,inplace=True)\n","\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","            print(\"Imaging File: \", i_filename)\n","            print(\"Dataset Name: \", dataset_name)\n","            print(\"Model Variable: \", model_var)\n","\n","            if 'Unnamed: 0' in df.columns:\n","                print('Unnamed: 0 in df')\n","            if 'Unnamed: 0' in df_i.columns:\n","                print('Unnamed: 0 in df_i')\n","            if 'Unnamed: 0' in df_with_i.columns:\n","                print('Unnamed: 0 in df_with_i')\n","\n","            ######################\n","            # SETTING ORIG_INDEX #\n","            ######################\n","\n","            df.reset_index(inplace=True)\n","            df_i.reset_index(inplace=True)\n","            df_with_i.reset_index(inplace=True)\n","\n","            df.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_i.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_with_i.rename(columns={'index':'orig_index'}, inplace=True)\n","\n","            #######################\n","            # ESTIMATOR SELECTION #\n","            #######################\n","\n","            if estimator == 1:\n","                model_estimator = \"Linear_SVR\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = LinearSVR(max_iter=10000)\n","            elif estimator == 2:\n","                model_estimator = \"SVR_Linear_Kernel\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'linear')\n","            elif estimator == 3:\n","                model_estimator = \"SVR_Nonlinear_RBF\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                             \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'rbf', gamma = 'auto')\n","            elif estimator == 4:\n","                model_estimator=\"XGBoost\"\n","                param_dist = {\"n_estimators\": randint(5,100),\n","                              \"learning_rate\": uniform(0.01,0.5),\n","                              \"gamma\": uniform(0.0,0.5),\n","                              \"max_depth\": randint(3,10),\n","                              \"min_child_weight\": randint(1,8),\n","                              \"subsample\": uniform(.6,0.4),\n","                              \"colsample_bytree\": uniform(.3,0.7),\n","                              \"reg_lambda\": uniform(0.001,5),\n","                              \"reg_alpha\": uniform(0.00001,1)}\n","                alg = XGBRegressor(n_jobs=6)\n","            elif estimator == 5:\n","                model_estimator = \"Linear_Regression\"\n","                alg = linear_model.LinearRegression(n_jobs=8)\n","            else:\n","                print(\"Estimator Not Selected\")\n","\n","            #print(\"Model Estimator: \", model_estimator)\n","\n","            #########################\n","            # DATASET PREPROCESSING #\n","            #########################\n","\n","            baselength = len(df.columns)\n","            base = [3,4]\n","            base2 = list(range(6,len(df.columns)))\n","            base.extend(base2)\n","            full1_preprocessor = make_column_transformer(\n","                (StandardScaler(), base),\n","                remainder='passthrough')\n","\n","            df.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","            if data == 3 or data == 4:\n","                df_with_i.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","\n","            i_index1 = index1 = df.columns.values[3]\n","            index2 = df.columns.values[6:].tolist()\n","            i_index3 = index3 = df.columns.values[5]\n","            i_index4 = index4 = df.columns.values[0:3].tolist()\n","            i_index5 = index5 = df.columns.values[4]\n","\n","            index2.extend(index4)\n","            index2.insert(0,index1)\n","            index2.insert(1,index5)\n","            index2.extend([index3])\n","\n","            df_scaled = full1_preprocessor.fit_transform(df)\n","            df_scaled = pd.DataFrame(data=df_scaled)\n","            df_scaled.columns = index2\n","\n","            if data == 3 or data == 4:\n","                base3 = list(range(6,len(df_i.columns)+baselength-2))\n","\n","                full = [3,4]\n","                full.extend(base3)\n","                full2_preprocessor = make_column_transformer(\n","                    (StandardScaler(), full),\n","                    remainder='passthrough')\n","\n","                i_index2 = df_with_i.columns.values[6:].tolist()\n","                i_index2.extend(i_index4)\n","                i_index2.insert(0,i_index1)\n","                i_index2.insert(1,i_index5)\n","                i_index2.extend([i_index3])\n","\n","                df_scaled_i = full2_preprocessor.fit_transform(df_with_i)\n","                df_scaled_i = pd.DataFrame(data=df_scaled_i)\n","                df_scaled_i.columns = i_index2\n","\n","            print(\"Data Preprocessed - Scaler Applied\")\n","\n","            ######################\n","            # FEATURES SELECTION #\n","            ######################\n","\n","            x1 = [1,2]\n","            x2 = [len(df_scaled.columns)-1]\n","            x3 = list(range(3,len(df_scaled.columns)-4))\n","            X1_vars = x1 + x2\n","            X2_vars = x1 + x2 + x3\n","\n","            if data == 3 or data ==4:\n","                x5 = [len(df_scaled_i.columns)-1]\n","                x4 = list(range(len(df_scaled.columns)-4,len(df_scaled_i.columns)-4))\n","                X3_vars = x1 + x5 + x3 + x4\n","\n","            X1 = df_scaled.iloc[:,X1_vars]\n","            y1 = df_scaled.iloc[:,0]\n","            if data == 1 or data == 2:\n","                groups1 = groups2 = df_scaled['famid']\n","            elif data == 3 or data == 4:\n","                groups1 = groups2 = df_scaled['fid']\n","            else:\n","                print('Dataset not set')\n","\n","            X2 = df_scaled.iloc[:,X2_vars]\n","            y2 = df_scaled.iloc[:,0]\n","            features_full1 = ['Age']\n","            for i in [X2_vars[1:]]:\n","                features_full1.extend(df_scaled.columns[i])\n","            features_demo = features_full1[0:3]\n","\n","            if data == 3 or data == 4:\n","                X3 = df_scaled_i.iloc[:,X3_vars]\n","                y3 = df_scaled_i.iloc[:,0]\n","                groups3 = df_scaled_i['fid'] # CHANGED FROM df_scaled.columns TO GET CORRECT # FAMID/GROUPS\n","                features_full2 = ['Age']\n","                for i in [X3_vars[1:]]:\n","                    features_full2.extend(df_scaled_i.columns[i])\n","\n","\n","            ##############################\n","            # CROSS-VALIDATION SELECTION #\n","            ##############################\n","\n","            inner_rstate = 9052017\n","            outer_rstate = 12301985\n","            #inner_rstate = np.random.RandomState(9052017)\n","            #outer_rstate = np.random.RandomState(12301985)\n","\n","            cv=GroupShuffleSplit\n","            #cv=GroupKFold\n","            cv_inner = cv(n_splits=20, test_size=0.2, random_state=inner_rstate)\n","            cv_outer = cv(n_splits=10, test_size=0.2, random_state=outer_rstate)\n","            n_iter=200\n","            n_iter_str = str(n_iter)\n","\n","            print(\"Features Selected\")\n","            print(\"Inner Training CV: \", cv_inner)\n","            print(\"Outer Testing CV: \", cv_outer)\n","\n","            #print(\"Dataset: \", dataset_name)\n","            #print(\"Model Variable: \", model_var)\n","            #print(\"Model Estimator: \", model_estimator)\n","            #print(\"Imaging File: \", i_filename)\n","\n","            ############################\n","            # SET SPECIFICITY OUTCOMES #\n","            ############################\n","\n","            if dataset_name == \"CLDRC_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=1265)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1265_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=1190)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1190_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL CLDRC CROSS-CONSTRUCT SPECIFICITY RESULTS (N=1675)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1675_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL HUB CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=214)\"\n","                df_outcomes = pd.read_csv(\"hub_214_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PATIAL HUB CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=209)\"\n","                df_outcomes = pd.read_csv(\"hub_209_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL HUB CROSS-CONSTRUCT SPECIFICITY RESULTS (N=324)\"\n","                df_outcomes = pd.read_csv(\"hub_324_specificity_outcomes.csv\")\n","            else:\n","                print(\"Dataset and Model Variables Not Set\")\n","\n","            print(\"\")\n","            print(\"Specificity Filename: \", specificity_file)\n","            print(\"\")\n","            r2 = df_outcomes['out_read_fs']\n","            i2 = df_outcomes['out_in_fs']\n","            h2 = df_outcomes['out_hi_fs']\n","            m2 = df_outcomes['out_math_fs']\n","\n","            ###############################\n","            ### NESTED CROSS-VALIDATORS ###\n","            ###############################\n","\n","            from sklearn.model_selection import ShuffleSplit\n","\n","            cvreg=ShuffleSplit\n","            cvreg_inner = cvreg(n_splits=20, test_size=0.2, random_state=123085)\n","            cvreg_outer = cvreg(n_splits=9, test_size=0.2, random_state=90517)\n","\n","            if (data == 3 or data == 4):\n","                #######################\n","                ### LassoCV Imaging ###\n","                #######################\n","\n","                print(\"LASSO_CV IMAGING\")\n","                search_time_start_lassoi = time.time()\n","                alphas_lassocvi = np.logspace(-4, -0.3, 100)\n","                lasso_cvi = LassoCV(alphas=alphas_lassocvi, cv=cvreg_inner, max_iter=100000, tol=0.0001, n_jobs=4)\n","                lasso_cvi_score = []\n","\n","                for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","                    lasso_cvi.fit(X3.values[trainCV], y3[trainCV])\n","                    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","                          format(k, lasso_cvi.alpha_, lasso_cvi.score(X3.values[testCV], y3[testCV])))\n","                    lasso_cvi_score.append(lasso_cvi.score(X3.values[testCV], y3[testCV]))\n","                search_time_stop_lassoi = time.time()\n","\n","                feature_list_lassoi = []\n","                for i in list(range(0,len(X3.columns))):\n","                    feature_list_lassoi.append([lasso_cvi.coef_[i], X3.columns[i]])\n","                    df_featimp_lassoi = pd.DataFrame(data=feature_list_lassoi, columns=['coef','feature'])\n","                df_featimp_lassoi.reindex(df_featimp_lassoi.coef.abs().sort_values(ascending=False).index)\n","                df_featimp_sum_lassoi = df_featimp_lassoi[(df_featimp_lassoi['coef'] > .001) | (df_featimp_lassoi['coef'] < -.001)]\n","                print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","\n","\n","                ###########################\n","                ### LassoLarsCV Imaging ###\n","                ###########################\n","\n","                print(\"LASSOLARS_CV IMAGING\")\n","                search_time_start_llarsi = time.time()\n","                llars_cvi = LassoLarsCV(cv=cvreg_inner, max_iter=10000, n_jobs=4)\n","                llars_cvi_score = []\n","\n","                for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","                    llars_cvi.fit(X3.values[trainCV], y3[trainCV])\n","                    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","                          format(k, llars_cvi.alpha_, llars_cvi.score(X3.values[testCV], y3[testCV])))\n","                    llars_cvi_score.append(llars_cvi.score(X3.values[testCV], y3[testCV]))\n","                search_time_stop_llarsi = time.time()\n","\n","                feature_list_lli = []\n","                for i in list(range(0,len(X3.columns))):\n","                    feature_list_lli.append([llars_cvi.coef_[i], X3.columns[i]])\n","                    df_featimp_lli = pd.DataFrame(data=feature_list_lli, columns=['coef','feature'])\n","                df_featimp_lli.reindex(df_featimp_lli.coef.abs().sort_values(ascending=False).index)\n","                df_featimp_sum_lli = df_featimp_lli[(df_featimp_lli['coef'] > .001) | (df_featimp_lli['coef'] < -.001)]\n","                print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","            else:\n","                print(\"Imaging regularization not available for CLDRC dataset\")\n","\n","            print('')\n","            print(\"LassoCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","                    format(np.mean(lasso_cvi_score), np.std(lasso_cvi_score), lasso_cvi.alpha_, search_time_stop_lassoi-search_time_start_lassoi))\n","            print(\"LLarsCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","                    format(np.mean(llars_cvi_score), np.std(llars_cvi_score), llars_cvi.alpha_, search_time_stop_llarsi-search_time_start_llarsi))\n","            print('')\n","            print('Lasso Imaging Weights')\n","            print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","            print('')\n","            print('LassoLars Imaging Weights')\n","            print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","            print('')\n","            print('')\n","            print('')\n","            print('')\n","            print('')\n","            print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YU5ocbH_97hm"},"outputs":[],"source":["with open(\"0.results_hubDKTvarloop_lassollars.txt\", 'w') as f:\n","    f.write(capture.stdout)"]},{"cell_type":"markdown","metadata":{"id":"S3LitvTw97hn"},"source":["# JUST IMAGING - DKT + FS5/TBSS Loop with Lasso-LLars"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QseSVrZy97hn"},"outputs":[],"source":["%%capture --no-stderr capture\n","\n","for j in range(4,5): # MODIFY to (1,5) for all 4 datasets\n","    for z in range(1,2): # MODIFY to (1,4) for all 3 variables\n","        for q in range(12,28): # MODIFY to (1,28) for all 27 imaging datasets\n","            data = j\n","            variable = z\n","            estimator = 1\n","            imaging = q\n","            #####################\n","            # DATASET SELECTION #\n","            #####################\n","\n","            if data == 1 and variable == 1:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 2:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 3:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 2 and variable == 1:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 2:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 3:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 3 and variable == 1:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 2:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 3:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 4 and variable == 1:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 2:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 3:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_hub.csv\", index_col=0)\n","            else:\n","                print(\"Data and Variable Not Selected\")\n","\n","            #######################\n","            # IMPORT IMAGING DATA #\n","            #######################\n","\n","            if imaging == 1:\n","                df_i = pd.read_csv('1.DKTarea_ICVresid_hub.csv'); i_filename = 'DKTarea_ICVresid_hub.csv' #1\n","            elif imaging == 2:\n","                df_i = pd.read_csv('1.DKTarea_SA_hub.csv'); i_filename = 'DKTarea_SA_hub.csv' #2\n","            elif imaging == 3:\n","                df_i = pd.read_csv('1.DKTarea_SAresid_hub.csv'); i_filename = 'DKTarea_SAresid_hub.csv' #3\n","            elif imaging == 4:\n","                df_i = pd.read_csv('1.DKTarea_asym_hub.csv'); i_filename = 'DKTarea_asym_hub.csv'\n","            elif imaging == 5:\n","                df_i = pd.read_csv('1.DKTarea_hub.csv'); i_filename = 'DKTarea_hub.csv'\n","            elif imaging == 6:\n","                df_i = pd.read_csv('1.DKTaseg_ICVresid_hub.csv'); i_filename = 'DKTaseg_ICVresid_hub.csv'\n","            elif imaging == 7:\n","                df_i = pd.read_csv('1.DKTaseg_hub.csv'); i_filename = 'DKTaseg_hub.csv' #4\n","            elif imaging == 8:\n","                df_i = pd.read_csv('1.DKTlgi_ICVresid_hub.csv'); i_filename = 'DKTlgi_ICVresid_hub.csv'\n","            elif imaging == 9:\n","                df_i = pd.read_csv('1.DKTlgi_SAresid_hub.csv'); i_filename = 'DKTlgi_SAresid_hub.csv'\n","            elif imaging == 10:\n","                df_i = pd.read_csv('1.DKTlgi_asym_hub.csv'); i_filename = 'DKTlgi_asym_hub.csv'\n","            elif imaging == 11:\n","                df_i = pd.read_csv('1.DKTlgi_hub.csv'); i_filename = 'DKTlgi_hub.csv'\n","            elif imaging == 12:\n","                df_i = pd.read_csv('1.DKTthick_ICVresid_hub.csv'); i_filename = 'DKTthick_ICVresid_hub.csv' #5\n","            elif imaging == 13:\n","                df_i = pd.read_csv('1.DKTthick_MT_hub.csv'); i_filename = 'DKTthick_MT_hub.csv' #6\n","            elif imaging == 14:\n","                df_i = pd.read_csv('1.DKTthick_MTresid_hub.csv'); i_filename = 'DKTthick_MTresid_hub.csv' #7\n","            elif imaging == 15:\n","                df_i = pd.read_csv('1.DKTthick_asym_hub.csv'); i_filename = 'DKTthick_asym_hub.csv'\n","            elif imaging == 16:\n","                df_i = pd.read_csv('1.DKTthick_hub.csv'); i_filename = 'DKTthick_hub.csv'\n","            elif imaging == 17:\n","                df_i = pd.read_csv('1.DKTvol_ICVresid_hub.csv'); i_filename = 'DKTvol_ICVresid_hub.csv'\n","            elif imaging == 18:\n","                df_i = pd.read_csv('1.DKTvol_SAresid_hub.csv'); i_filename = 'DKTvol_SAresid_hub.csv'\n","            elif imaging == 19:\n","                df_i = pd.read_csv('1.DKTvol_asym_hub.csv'); i_filename = 'DKTvol_asym_hub.csv'\n","            elif imaging == 20:\n","                df_i = pd.read_csv('1.DKTvol_hub.csv'); i_filename = 'DKTvol_hub.csv'\n","            elif imaging == 21:\n","                df_i = pd.read_csv('1.fs5new.csv', na_values='NaN', index_col=0); i_filename = \"fs5_labels\"\n","            elif imaging == 22:\n","                df_i = pd.read_csv('1.tbssall.csv', na_values='NaN', index_col=0); i_filename = \"tbss\"\n","            elif imaging == 23:\n","                df_i = pd.read_csv('1.aseg.csv', na_values='NaN', index_col=0); i_filename = \"aseg\"\n","            elif imaging == 24:\n","                df_i = pd.read_csv('1.fs5new_tbssall.csv', na_values='NaN', index_col=0); i_filename = \"fs5_tbss\"\n","            elif imaging == 25:\n","                df_i = pd.read_csv('1.fs5new_aseg.csv', na_values='NaN', index_col=0); i_filename = \"fs5_aseg\"\n","            elif imaging == 26:\n","                df_i = pd.read_csv('1.tbssall_aseg.csv', na_values='NaN', index_col=0); i_filename = \"tbss_aseg\"\n","            elif imaging == 27:\n","                df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0); i_filename = \"fs5_tbss_aseg\"\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","\n","            ################################\n","            # COMBINE BEHAVIORAL & IMAGING #\n","            ################################\n","\n","            if data == 3 or data == 4:\n","                df_with_i = df.merge(df_i, on='scanid1')\n","                df_with_i = df_with_i.dropna()\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df.rename(columns={'Unnamed: 0_x':'Unnamed: 0'}, inplace=True)\n","            #    df_with_i.drop(df_with_i.columns[13], axis=1, inplace=True)\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df_with_i.reset_index(drop=True,inplace=True)\n","\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","            print(\"Imaging File: \", i_filename)\n","            print(\"Dataset Name: \", dataset_name)\n","            print(\"Model Variable: \", model_var)\n","\n","            if 'Unnamed: 0' in df.columns:\n","                print('Unnamed: 0 in df')\n","            if 'Unnamed: 0' in df_i.columns:\n","                print('Unnamed: 0 in df_i')\n","            if 'Unnamed: 0' in df_with_i.columns:\n","                print('Unnamed: 0 in df_with_i')\n","\n","            ######################\n","            # SETTING ORIG_INDEX #\n","            ######################\n","\n","            df.reset_index(inplace=True)\n","            df_i.reset_index(inplace=True)\n","            df_with_i.reset_index(inplace=True)\n","\n","            df.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_i.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_with_i.rename(columns={'index':'orig_index'}, inplace=True)\n","\n","            #######################\n","            # ESTIMATOR SELECTION #\n","            #######################\n","\n","            if estimator == 1:\n","                model_estimator = \"Linear_SVR\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = LinearSVR(max_iter=10000)\n","            elif estimator == 2:\n","                model_estimator = \"SVR_Linear_Kernel\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'linear')\n","            elif estimator == 3:\n","                model_estimator = \"SVR_Nonlinear_RBF\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                             \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'rbf', gamma = 'auto')\n","            elif estimator == 4:\n","                model_estimator=\"XGBoost\"\n","                param_dist = {\"n_estimators\": randint(5,100),\n","                              \"learning_rate\": uniform(0.01,0.5),\n","                              \"gamma\": uniform(0.0,0.5),\n","                              \"max_depth\": randint(3,10),\n","                              \"min_child_weight\": randint(1,8),\n","                              \"subsample\": uniform(.6,0.4),\n","                              \"colsample_bytree\": uniform(.3,0.7),\n","                              \"reg_lambda\": uniform(0.001,5),\n","                              \"reg_alpha\": uniform(0.00001,1)}\n","                alg = XGBRegressor(n_jobs=6)\n","            elif estimator == 5:\n","                model_estimator = \"Linear_Regression\"\n","                alg = linear_model.LinearRegression(n_jobs=8)\n","            else:\n","                print(\"Estimator Not Selected\")\n","\n","            #print(\"Model Estimator: \", model_estimator)\n","\n","            #########################\n","            # DATASET PREPROCESSING #\n","            #########################\n","\n","            baselength = len(df.columns)\n","            base = [3,4]\n","            base2 = list(range(6,len(df.columns)))\n","            base.extend(base2)\n","            full1_preprocessor = make_column_transformer(\n","                (StandardScaler(), base),\n","                remainder='passthrough')\n","\n","            df.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","            if data == 3 or data == 4:\n","                df_with_i.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","\n","            i_index1 = index1 = df.columns.values[3]\n","            index2 = df.columns.values[6:].tolist()\n","            i_index3 = index3 = df.columns.values[5]\n","            i_index4 = index4 = df.columns.values[0:3].tolist()\n","            i_index5 = index5 = df.columns.values[4]\n","\n","            index2.extend(index4)\n","            index2.insert(0,index1)\n","            index2.insert(1,index5)\n","            index2.extend([index3])\n","\n","            df_scaled = full1_preprocessor.fit_transform(df)\n","            df_scaled = pd.DataFrame(data=df_scaled)\n","            df_scaled.columns = index2\n","\n","            if data == 3 or data == 4:\n","                base3 = list(range(6,len(df_i.columns)+baselength-2))\n","\n","                full = [3,4]\n","                full.extend(base3)\n","                full2_preprocessor = make_column_transformer(\n","                    (StandardScaler(), full),\n","                    remainder='passthrough')\n","\n","                i_index2 = df_with_i.columns.values[6:].tolist()\n","                i_index2.extend(i_index4)\n","                i_index2.insert(0,i_index1)\n","                i_index2.insert(1,i_index5)\n","                i_index2.extend([i_index3])\n","\n","                df_scaled_i = full2_preprocessor.fit_transform(df_with_i)\n","                df_scaled_i = pd.DataFrame(data=df_scaled_i)\n","                df_scaled_i.columns = i_index2\n","\n","            print(\"Data Preprocessed - Scaler Applied\")\n","\n","            ######################\n","            # FEATURES SELECTION #    ####### READ: X3.columns[9:] ; IN & HI: X3.columns[11:]\n","            ######################\n","\n","            x1 = [1,2]\n","            x2 = [len(df_scaled.columns)-1]\n","            x3 = list(range(3,len(df_scaled.columns)-4))\n","            X1_vars = x1 + x2\n","            X2_vars = x1 + x2 + x3\n","\n","            if data == 3 or data ==4:\n","                x5 = [len(df_scaled_i.columns)-1]\n","                x4 = list(range(len(df_scaled.columns)-4,len(df_scaled_i.columns)-4))\n","                X3_vars = x1 + x5 + x3 + x4\n","\n","            X1 = df_scaled.iloc[:,X1_vars]\n","            y1 = df_scaled.iloc[:,0]\n","            if data == 1 or data == 2:\n","                groups1 = groups2 = df_scaled['famid']\n","            elif data == 3 or data == 4:\n","                groups1 = groups2 = df_scaled['fid']\n","            else:\n","                print('Dataset not set')\n","\n","            X2 = df_scaled.iloc[:,X2_vars]\n","            y2 = df_scaled.iloc[:,0]\n","            features_full1 = ['Age']\n","            for i in [X2_vars[1:]]:\n","                features_full1.extend(df_scaled.columns[i])\n","            features_demo = features_full1[0:3]\n","\n","            #####################################################\n","            #####################################################\n","            #####################################################\n","            ### CRITICAL EDITS TO REMOVE BEHAVIORAL VARIABLES ###\n","            #####################################################\n","            #####################################################\n","            #####################################################\n","\n","            if data == 3 or data == 4:\n","                if variable == 1:\n","                    X3 = df_scaled_i.iloc[:,X3_vars[10:]]\n","                elif variable == 2 or variable == 3:\n","                    X3 = df_scaled_i.iloc[:,X3_vars[12:]]\n","                else:\n","                    print('VARIABLE SELECTION ERROR')\n","                y3 = df_scaled_i.iloc[:,0]\n","                groups3 = df_scaled_i['fid'] # CHANGED FROM df_scaled.columns TO GET CORRECT # FAMID/GROUPS\n","                features_full2 = [] # REMOVED 'Age' from variable column names in features_full2\n","                if variable == 1:\n","                    for i in [X3_vars[10:]]:\n","                        features_full2.extend(df_scaled_i.columns[i])\n","                elif variable == 2 or variable == 3:\n","                    for i in [X3_vars[12:]]:\n","                        features_full2.extend(df_scaled_i.columns[i])\n","                else:\n","                    print('VARIABLE COLUMN NAME MATCHING ERROR')\n","\n","\n","            ##############################\n","            # CROSS-VALIDATION SELECTION #\n","            ##############################\n","\n","            inner_rstate = 9052017\n","            outer_rstate = 12301985\n","            #inner_rstate = np.random.RandomState(9052017)\n","            #outer_rstate = np.random.RandomState(12301985)\n","\n","            cv=GroupShuffleSplit\n","            #cv=GroupKFold\n","            cv_inner = cv(n_splits=20, test_size=0.2, random_state=inner_rstate)\n","            cv_outer = cv(n_splits=10, test_size=0.2, random_state=outer_rstate)\n","            n_iter=200\n","            n_iter_str = str(n_iter)\n","\n","            print(\"Features Selected\")\n","            print(\"Inner Training CV: \", cv_inner)\n","            print(\"Outer Testing CV: \", cv_outer)\n","\n","            #print(\"Dataset: \", dataset_name)\n","            #print(\"Model Variable: \", model_var)\n","            #print(\"Model Estimator: \", model_estimator)\n","            #print(\"Imaging File: \", i_filename)\n","\n","            ############################\n","            # SET SPECIFICITY OUTCOMES #\n","            ############################\n","\n","            if dataset_name == \"CLDRC_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=1265)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1265_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=1190)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1190_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL CLDRC CROSS-CONSTRUCT SPECIFICITY RESULTS (N=1675)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1675_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL HUB CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=214)\"\n","                df_outcomes = pd.read_csv(\"hub_214_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PATIAL HUB CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=209)\"\n","                df_outcomes = pd.read_csv(\"hub_209_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL HUB CROSS-CONSTRUCT SPECIFICITY RESULTS (N=324)\"\n","                df_outcomes = pd.read_csv(\"hub_324_specificity_outcomes.csv\")\n","            else:\n","                print(\"Dataset and Model Variables Not Set\")\n","\n","            print(\"\")\n","            print(\"Specificity Filename: \", specificity_file)\n","            print(\"\")\n","            r2 = df_outcomes['out_read_fs']\n","            i2 = df_outcomes['out_in_fs']\n","            h2 = df_outcomes['out_hi_fs']\n","            m2 = df_outcomes['out_math_fs']\n","\n","            ###############################\n","            ### NESTED CROSS-VALIDATORS ###\n","            ###############################\n","\n","            from sklearn.model_selection import ShuffleSplit\n","\n","            cvreg=ShuffleSplit\n","            cvreg_inner = cvreg(n_splits=20, test_size=0.2, random_state=123085)\n","            cvreg_outer = cvreg(n_splits=9, test_size=0.2, random_state=90517)\n","\n","            if (data == 3 or data == 4):\n","                #######################\n","                ### LassoCV Imaging ###\n","                #######################\n","\n","###                print(\"LASSO_CV IMAGING\")\n","###                search_time_start_lassoi = time.time()\n","###                alphas_lassocvi = np.logspace(-4, -0.3, 100)\n","###                lasso_cvi = LassoCV(alphas=alphas_lassocvi, cv=cvreg_inner, max_iter=10000, tol=0.0001, n_jobs=4)\n","###                lasso_cvi_score = []\n","\n","###                for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","###                    lasso_cvi.fit(X3.values[trainCV], y3[trainCV])\n","###                   print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","###                          format(k, lasso_cvi.alpha_, lasso_cvi.score(X3.values[testCV], y3[testCV])))\n","###                    lasso_cvi_score.append(lasso_cvi.score(X3.values[testCV], y3[testCV]))\n","###                search_time_stop_lassoi = time.time()\n","\n","###                feature_list_lassoi = []\n","###                for i in list(range(0,len(X3.columns))):\n","###                    feature_list_lassoi.append([lasso_cvi.coef_[i], X3.columns[i]])\n","###                    df_featimp_lassoi = pd.DataFrame(data=feature_list_lassoi, columns=['coef','feature'])\n","###                df_featimp_lassoi.reindex(df_featimp_lassoi.coef.abs().sort_values(ascending=False).index)\n","###                df_featimp_sum_lassoi = df_featimp_lassoi[(df_featimp_lassoi['coef'] > .001) | (df_featimp_lassoi['coef'] < -.001)]\n","###                print('')\n","#                print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","\n","\n","                ###########################\n","                ### LassoLarsCV Imaging ###\n","                ###########################\n","\n","                print(\"LASSOLARS_CV IMAGING\")\n","                search_time_start_llarsi = time.time()\n","                llars_cvi = LassoLarsCV(cv=cvreg_inner, max_iter=10000, n_jobs=4)\n","                llars_cvi_score = []\n","\n","                for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","                    llars_cvi.fit(X3.values[trainCV], y3[trainCV])\n","                    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","                          format(k, llars_cvi.alpha_, llars_cvi.score(X3.values[testCV], y3[testCV])))\n","                    llars_cvi_score.append(llars_cvi.score(X3.values[testCV], y3[testCV]))\n","                search_time_stop_llarsi = time.time()\n","\n","                feature_list_lli = []\n","                for i in list(range(0,len(X3.columns))):\n","                    feature_list_lli.append([llars_cvi.coef_[i], X3.columns[i]])\n","                    df_featimp_lli = pd.DataFrame(data=feature_list_lli, columns=['coef','feature'])\n","                df_featimp_lli.reindex(df_featimp_lli.coef.abs().sort_values(ascending=False).index)\n","                df_featimp_sum_lli = df_featimp_lli[(df_featimp_lli['coef'] > .001) | (df_featimp_lli['coef'] < -.001)]\n","                print('')\n","#                print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","            else:\n","                print(\"Imaging regularization not available for CLDRC dataset\")\n","\n","            print('')\n","###            print(\"LassoCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","###                    format(np.mean(lasso_cvi_score), np.std(lasso_cvi_score), lasso_cvi.alpha_, search_time_stop_lassoi-search_time_start_lassoi))\n","            print(\"LLarsCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","                    format(np.mean(llars_cvi_score), np.std(llars_cvi_score), llars_cvi.alpha_, search_time_stop_llarsi-search_time_start_llarsi))\n","            print('')\n","###            print('Lasso Imaging Weights')\n","###            print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","###            print('')\n","            print('LassoLars Imaging Weights')\n","            print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","            print('')\n","            print('')\n","            print('')\n","            print('')\n","            print('')\n","            print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Cbw-DzJ97hp"},"outputs":[],"source":["with open(\"0.results_imagingonly_hubDKTfs5varloop_llars_READ_5.2.20.txt\", 'w') as f:\n","    f.write(capture.stdout)"]},{"cell_type":"markdown","metadata":{"id":"bNljycp797hp"},"source":["# LASSO-LLARS FS5 Dataset Variable Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xI_9q6c97hp"},"outputs":[],"source":["%%capture --no-stderr capture\n","\n","for j in range(4,5): # MODIFY to (1,5) for all 4 datasets\n","    for z in range(1,4): # MODIFY to (1,4) for all 3 variables\n","        data = j\n","        variable = z\n","        estimator = 1\n","        imaging = 7\n","        #####################\n","        # DATASET SELECTION #\n","        #####################\n","\n","        if data == 1 and variable == 1:\n","            dataset_name = \"CLDRC_DropNA\"\n","            model_var = \"READ\"\n","            df = pd.read_csv(\"df_vars_readfs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 1 and variable == 2:\n","            dataset_name = \"CLDRC_DropNA\"\n","            model_var = \"IN\"\n","            df = pd.read_csv(\"df_vars_infs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 1 and variable == 3:\n","            dataset_name = \"CLDRC_DropNA\"\n","            model_var = \"HI\"\n","            df = pd.read_csv(\"df_vars_hifs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 2 and variable == 1:\n","            dataset_name = \"CLDRC_Imputed\"\n","            model_var = \"READ\"\n","            df = pd.read_csv(\"df_vars_readfs_imputed_cldrc.csv\", index_col=0)\n","        elif data == 2 and variable == 2:\n","            dataset_name = \"CLDRC_Imputed\"\n","            model_var = \"IN\"\n","            df = pd.read_csv(\"df_vars_infs_imputed_cldrc.csv\", index_col=0)\n","        elif data == 2 and variable == 3:\n","            dataset_name = \"CLDRC_Imputed\"\n","            model_var = \"HI\"\n","            df = pd.read_csv(\"df_vars_hifs_imputed_cldrc.csv\", index_col=0)\n","        elif data == 3 and variable == 1:\n","            dataset_name = \"Hub_DropNA\"\n","            model_var = \"READ\"\n","            df = pd.read_csv(\"df_vars_readfs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 3 and variable == 2:\n","            dataset_name = \"Hub_DropNA\"\n","            model_var = \"IN\"\n","            df = pd.read_csv(\"df_vars_infs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 3 and variable == 3:\n","            dataset_name = \"Hub_DropNA\"\n","            model_var = \"HI\"\n","            df = pd.read_csv(\"df_vars_hifs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","        elif data == 4 and variable == 1:\n","            dataset_name = \"Hub_Imputed\"\n","            model_var = \"READ\"\n","            df = pd.read_csv(\"df_vars_readfs_imputed_hub.csv\", index_col=0)\n","        elif data == 4 and variable == 2:\n","            dataset_name = \"Hub_Imputed\"\n","            model_var = \"IN\"\n","            df = pd.read_csv(\"df_vars_infs_imputed_hub.csv\", index_col=0)\n","        elif data == 4 and variable == 3:\n","            dataset_name = \"Hub_Imputed\"\n","            model_var = \"HI\"\n","            df = pd.read_csv(\"df_vars_hifs_imputed_hub.csv\", index_col=0)\n","        else:\n","            print(\"Data and Variable Not Selected\")\n","\n","        #######################\n","        # IMPORT IMAGING DATA #\n","        #######################\n","\n","        if (data == 3 or data == 4):\n","            if imaging == 1:\n","                df_i = pd.read_csv('1.fs5new.csv', na_values='NaN', index_col=0)\n","                i_filename = \"fs5_labels\"\n","            elif imaging == 2:\n","                df_i = pd.read_csv('1.tbssall.csv', na_values='NaN', index_col=0)                      # CAN REMOVE ALL FROM tbss\n","                i_filename = \"tbss\"\n","            elif imaging == 3:\n","                df_i = pd.read_csv('1.aseg.csv', na_values='NaN', index_col=0)\n","                i_filename = \"aseg\"\n","            elif imaging == 4:\n","                df_i = pd.read_csv('1.fs5new_tbssall.csv', na_values='NaN', index_col=0)                     # CAN REMOVE ALL FROM tbss\n","        #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","        #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","        #        df_i = df_fs5.merge(df_tbss, on='scanid1')\n","                i_filename = \"fs5_tbss\"\n","            elif imaging == 5:\n","                df_i = pd.read_csv('1.fs5new_aseg.csv', na_values='NaN', index_col=0)\n","        #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","        #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","                df_i = df_fs5.merge(df_aseg, on='scanid1')\n","                i_filename = \"fs5_aseg\"\n","            elif imaging == 6:\n","                df_i = pd.read_csv('1.tbssall_aseg.csv', na_values='NaN', index_col=0)                    # CAN REMOVE ALL FROM tbss\n","        #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","        #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","        #        df_i = df_tbss.merge(df_aseg, on='scanid1')\n","                i_filename = \"tbss_aseg\"\n","            elif imaging == 7:\n","                df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)                # CAN REMOVE ALL FROM tbss\n","        #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","        #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","        #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","        #        df_pre_i = df_fs5.merge(df_tbss, on='scanid1')\n","        #        df_i = df_pre_i.merge(df_aseg, on='scanid1')\n","                i_filename = \"fs5_tbss_aseg\"\n","            else:\n","                print('No imaging selected for Hub analysis')\n","        else:\n","            df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","            df_with_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","            print('No imaging in CLDRC')\n","\n","        ################################\n","        # COMBINE BEHAVIORAL & IMAGING #\n","        ################################\n","\n","        if data == 3 or data == 4:\n","            df_with_i = df.merge(df_i, on='scanid1')\n","            df_with_i = df_with_i.dropna()\n","        #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","        #    df.rename(columns={'Unnamed: 0_x':'Unnamed: 0'}, inplace=True)\n","        #    df_with_i.drop(df_with_i.columns[13], axis=1, inplace=True)\n","        #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","        #    df_with_i.reset_index(drop=True,inplace=True)\n","\n","        else:\n","            i_filename = 'No_Imaging_in_Model'\n","        print(\"Imaging File: \", i_filename)\n","        print(\"Dataset Name: \", dataset_name)\n","        print(\"Model Variable: \", model_var)\n","\n","        if 'Unnamed: 0' in df.columns:\n","            print('Unnamed: 0 in df')\n","        if 'Unnamed: 0' in df_i.columns:\n","            print('Unnamed: 0 in df_i')\n","        if 'Unnamed: 0' in df_with_i.columns:\n","            print('Unnamed: 0 in df_with_i')\n","\n","        ######################\n","        # SETTING ORIG_INDEX #\n","        ######################\n","\n","        df.reset_index(inplace=True)\n","        df_i.reset_index(inplace=True)\n","        df_with_i.reset_index(inplace=True)\n","\n","        df.rename(columns={'index':'orig_index'}, inplace=True)\n","        df_i.rename(columns={'index':'orig_index'}, inplace=True)\n","        df_with_i.rename(columns={'index':'orig_index'}, inplace=True)\n","\n","        #######################\n","        # ESTIMATOR SELECTION #\n","        #######################\n","\n","        if estimator == 1:\n","            model_estimator = \"Linear_SVR\"\n","            param_dist = {\"C\": uniform(.0001,5),\n","                          \"epsilon\": uniform(0,1)}\n","            alg = LinearSVR(max_iter=10000)\n","        elif estimator == 2:\n","            model_estimator = \"SVR_Linear_Kernel\"\n","            param_dist = {\"C\": uniform(.0001,5),\n","                          \"epsilon\": uniform(0,1)}\n","            alg = SVR(kernel = 'linear')\n","        elif estimator == 3:\n","            model_estimator = \"SVR_Nonlinear_RBF\"\n","            param_dist = {\"C\": uniform(.0001,5),\n","                         \"epsilon\": uniform(0,1)}\n","            alg = SVR(kernel = 'rbf', gamma = 'auto')\n","        elif estimator == 4:\n","            model_estimator=\"XGBoost\"\n","            param_dist = {\"n_estimators\": randint(5,100),\n","                          \"learning_rate\": uniform(0.01,0.5),\n","                          \"gamma\": uniform(0.0,0.5),\n","                          \"max_depth\": randint(3,10),\n","                          \"min_child_weight\": randint(1,8),\n","                          \"subsample\": uniform(.6,0.4),\n","                          \"colsample_bytree\": uniform(.3,0.7),\n","                          \"reg_lambda\": uniform(0.001,5),\n","                          \"reg_alpha\": uniform(0.00001,1)}\n","            alg = XGBRegressor(n_jobs=6)\n","        elif estimator == 5:\n","            model_estimator = \"Linear_Regression\"\n","            alg = linear_model.LinearRegression(n_jobs=8)\n","        else:\n","            print(\"Estimator Not Selected\")\n","\n","        print(\"Model Estimator: \", model_estimator)\n","\n","        #########################\n","        # DATASET PREPROCESSING #\n","        #########################\n","\n","        baselength = len(df.columns)\n","        base = [3,4]\n","        base2 = list(range(6,len(df.columns)))\n","        base.extend(base2)\n","        full1_preprocessor = make_column_transformer(\n","            (StandardScaler(), base),\n","            remainder='passthrough')\n","\n","        df.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","        ###################################\n","        ### POSSIBLY SHOULD BE EXCLUDED ###\n","        ###################################\n","        if data == 3 or data == 4:\n","            df_with_i.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","        ###################################\n","        ### POSSIBLY SHOULD BE EXCLUDED ###\n","        ###################################\n","\n","        i_index1 = index1 = df.columns.values[3]\n","        index2 = df.columns.values[6:].tolist()\n","        i_index3 = index3 = df.columns.values[5]\n","        i_index4 = index4 = df.columns.values[0:3].tolist()\n","        i_index5 = index5 = df.columns.values[4]\n","\n","        index2.extend(index4)\n","        index2.insert(0,index1)\n","        index2.insert(1,index5)\n","        index2.extend([index3])\n","\n","        df_scaled = full1_preprocessor.fit_transform(df)\n","        df_scaled = pd.DataFrame(data=df_scaled)\n","        df_scaled.columns = index2\n","\n","        if data == 3 or data == 4:\n","            base3 = list(range(6,len(df_i.columns)+baselength-2))\n","\n","            full = [3,4]\n","            full.extend(base3)\n","            full2_preprocessor = make_column_transformer(\n","                (StandardScaler(), full),\n","                remainder='passthrough')\n","\n","            i_index2 = df_with_i.columns.values[6:].tolist()\n","            i_index2.extend(i_index4)\n","            i_index2.insert(0,i_index1)\n","            i_index2.insert(1,i_index5)\n","            i_index2.extend([i_index3])\n","\n","            df_scaled_i = full2_preprocessor.fit_transform(df_with_i)\n","            df_scaled_i = pd.DataFrame(data=df_scaled_i)\n","            df_scaled_i.columns = i_index2\n","\n","        print(\"Data Preprocessed - Scaler Applied\")\n","\n","        ######################\n","        # FEATURES SELECTION #\n","        ######################\n","\n","        x1 = [1,2]\n","        x2 = [len(df_scaled.columns)-1]\n","        x3 = list(range(3,len(df_scaled.columns)-4))\n","        X1_vars = x1 + x2\n","        X2_vars = x1 + x2 + x3\n","\n","        if data == 3 or data ==4:\n","            x5 = [len(df_scaled_i.columns)-1]\n","            x4 = list(range(len(df_scaled.columns)-4,len(df_scaled_i.columns)-4))\n","            X3_vars = x1 + x5 + x3 + x4\n","\n","        X1 = df_scaled.iloc[:,X1_vars]\n","        y1 = df_scaled.iloc[:,0]\n","        if data == 1 or data == 2:\n","            groups1 = groups2 = df_scaled['famid']\n","        elif data == 3 or data == 4:\n","            groups1 = groups2 = df_scaled['fid']\n","        else:\n","            print('Dataset not set')\n","\n","        X2 = df_scaled.iloc[:,X2_vars]\n","        y2 = df_scaled.iloc[:,0]\n","        features_full1 = ['Age']\n","        for i in [X2_vars[1:]]:\n","            features_full1.extend(df_scaled.columns[i])\n","        features_demo = features_full1[0:3]\n","\n","        if data == 3 or data == 4:\n","            X3 = df_scaled_i.iloc[:,X3_vars]\n","            y3 = df_scaled_i.iloc[:,0]\n","            groups3 = df_scaled_i['fid'] # CHANGED FROM df_scaled.columns TO GET CORRECT # FAMID/GROUPS\n","            features_full2 = ['Age']\n","            for i in [X3_vars[1:]]:\n","                features_full2.extend(df_scaled_i.columns[i])\n","\n","\n","        ##############################\n","        # CROSS-VALIDATION SELECTION #\n","        ##############################\n","\n","        inner_rstate = 9052017\n","        outer_rstate = 12301985\n","        #inner_rstate = np.random.RandomState(9052017)\n","        #outer_rstate = np.random.RandomState(12301985)\n","\n","        cv=GroupShuffleSplit\n","        #cv=GroupKFold\n","        cv_inner = cv(n_splits=20, test_size=0.2, random_state=inner_rstate)\n","        cv_outer = cv(n_splits=10, test_size=0.2, random_state=outer_rstate)\n","        n_iter=200\n","        n_iter_str = str(n_iter)\n","\n","        print(\"Features Selected\")\n","        print(\"Inner Training CV: \", cv_inner)\n","        print(\"Outer Testing CV: \", cv_outer)\n","\n","        print(\"Dataset: \", dataset_name)\n","        print(\"Model Variable: \", model_var)\n","        print(\"Model Estimator: \", model_estimator)\n","        print(\"Imaging File: \", i_filename)\n","\n","        ############################\n","        # SET SPECIFICITY OUTCOMES #\n","        ############################\n","\n","        if dataset_name == \"CLDRC_DropNA\" and model_var == \"READ\":\n","            specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=1265)\"\n","            df_outcomes = pd.read_csv(\"cldrc_1265_specificity_outcomes.csv\")\n","        elif dataset_name == \"CLDRC_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","            specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=1190)\"\n","            df_outcomes = pd.read_csv(\"cldrc_1190_specificity_outcomes.csv\")\n","        elif dataset_name == \"CLDRC_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","            specificity_file = \"FULL CLDRC CROSS-CONSTRUCT SPECIFICITY RESULTS (N=1675)\"\n","            df_outcomes = pd.read_csv(\"cldrc_1675_specificity_outcomes.csv\")\n","        elif dataset_name == \"Hub_DropNA\" and model_var == \"READ\":\n","            specificity_file = \"PARTIAL HUB CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=214)\"\n","            df_outcomes = pd.read_csv(\"hub_214_specificity_outcomes.csv\")\n","        elif dataset_name == \"Hub_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","            specificity_file = \"PATIAL HUB CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=209)\"\n","            df_outcomes = pd.read_csv(\"hub_209_specificity_outcomes.csv\")\n","        elif dataset_name == \"Hub_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","            specificity_file = \"FULL HUB CROSS-CONSTRUCT SPECIFICITY RESULTS (N=324)\"\n","            df_outcomes = pd.read_csv(\"hub_324_specificity_outcomes.csv\")\n","        else:\n","            print(\"Dataset and Model Variables Not Set\")\n","\n","        print(\"\")\n","        print(\"Specificity Filename: \", specificity_file)\n","        print(\"\")\n","        r2 = df_outcomes['out_read_fs']\n","        i2 = df_outcomes['out_in_fs']\n","        h2 = df_outcomes['out_hi_fs']\n","        m2 = df_outcomes['out_math_fs']\n","\n","        ###############################\n","        ### NESTED CROSS-VALIDATORS ###\n","        ###############################\n","\n","        from sklearn.model_selection import ShuffleSplit\n","\n","        cvreg=ShuffleSplit\n","        cvreg_inner = cvreg(n_splits=20, test_size=0.2, random_state=123085)\n","        cvreg_outer = cvreg(n_splits=9, test_size=0.2, random_state=90517)\n","\n","        ##########################\n","        ### LassoCV Behavioral ###\n","        ##########################\n","\n","###        from sklearn.linear_model import LassoCV\n","\n","###        print(\"LASSO_CV BEHAVIOR\")\n","###        search_time_start_lassob = time.time()\n","###        alphas_lassocvb = np.logspace(-4, -0.3, 100)\n","###        lasso_cvb = LassoCV(alphas=alphas_lassocvb, cv=cvreg_inner, max_iter=100000, tol=0.0001, n_jobs=4)\n","###        lasso_cvb_score = []\n","\n","###        for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","###            lasso_cvb.fit(X2.values[trainCV], y2[trainCV])\n","###            print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","###                  format(k, lasso_cvb.alpha_, lasso_cvb.score(X2.values[testCV], y2[testCV])))\n","###            lasso_cvb_score.append(lasso_cvb.score(X2.values[testCV], y2[testCV]))\n","###        search_time_stop_lassob = time.time()\n","\n","###        feature_list_lassob = []\n","###        for i in list(range(0,len(X2.columns))):\n","###            feature_list_lassob.append([lasso_cvb.coef_[i], X2.columns[i]])\n","###            df_featimp_lassob = pd.DataFrame(data=feature_list_lassob, columns=['coef','feature'])\n","###        df_featimp_lassob.reindex(df_featimp_lassob.coef.abs().sort_values(ascending=False).index)\n","###        df_featimp_sum_lassob = df_featimp_lassob[(df_featimp_lassob['coef'] > .001) | (df_featimp_lassob['coef'] < -.001)]\n","###        print(df_featimp_sum_lassob.reindex(df_featimp_sum_lassob.coef.abs().sort_values(ascending=False).index))\n","###        print(\"\")\n","###        print(\"\")\n","\n","\n","        ##############################\n","        ### LassoLarsCV Behavioral ###\n","        ##############################\n","\n","        from sklearn.linear_model import LassoLarsCV\n","\n","        print(\"LASSOLARS_CV BEHAVIOR\")\n","        search_time_start_llarsb = time.time()\n","        llars_cvb = LassoLarsCV(cv=cvreg_inner, max_iter=100000, n_jobs=4)\n","        llars_cvb_score = []\n","\n","        for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X1, y1, groups=groups1)):\n","            llars_cvb.fit(X1.values[trainCV], y1[trainCV])\n","            print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","                  format(k, llars_cvb.alpha_, llars_cvb.score(X1.values[testCV], y1[testCV])))\n","            llars_cvb_score.append(llars_cvb.score(X1.values[testCV], y1[testCV]))\n","        search_time_stop_llarsb = time.time()\n","\n","\n","        feature_list_llb = []\n","        for i in list(range(0,len(X1.columns))):\n","            feature_list_llb.append([llars_cvb.coef_[i], X1.columns[i]])\n","            df_featimp_llb = pd.DataFrame(data=feature_list_llb, columns=['coef','feature'])\n","        df_featimp_llb.reindex(df_featimp_llb.coef.abs().sort_values(ascending=False).index)\n","        df_featimp_sum_llb = df_featimp_llb[(df_featimp_llb['coef'] > .001) | (df_featimp_llb['coef'] < -.001)]\n","        print(df_featimp_sum_llb.reindex(df_featimp_sum_llb.coef.abs().sort_values(ascending=False).index))\n","        print(\"\")\n","        print(\"\")\n","\n","\n","###        if (data == 3 or data == 4):\n","            #######################\n","            ### LassoCV Imaging ###\n","            #######################\n","\n","###            print(\"LASSO_CV IMAGING\")\n","###            search_time_start_lassoi = time.time()\n","###            alphas_lassocvi = np.logspace(-4, -0.3, 100)\n","###            lasso_cvi = LassoCV(alphas=alphas_lassocvi, cv=cvreg_inner, max_iter=100000, tol=0.0001, n_jobs=4)\n","###            lasso_cvi_score = []\n","\n","###            for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","###                lasso_cvi.fit(X3.values[trainCV], y3[trainCV])\n","###                print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","###                      format(k, lasso_cvi.alpha_, lasso_cvi.score(X3.values[testCV], y3[testCV])))\n","###                lasso_cvi_score.append(lasso_cvi.score(X3.values[testCV], y3[testCV]))\n","###            search_time_stop_lassoi = time.time()\n","\n","###            feature_list_lassoi = []\n","###            for i in list(range(0,len(X3.columns))):\n","###                feature_list_lassoi.append([lasso_cvi.coef_[i], X3.columns[i]])\n","###                df_featimp_lassoi = pd.DataFrame(data=feature_list_lassoi, columns=['coef','feature'])\n","###            df_featimp_lassoi.reindex(df_featimp_lassoi.coef.abs().sort_values(ascending=False).index)\n","###            df_featimp_sum_lassoi = df_featimp_lassoi[(df_featimp_lassoi['coef'] > .001) | (df_featimp_lassoi['coef'] < -.001)]\n","###            print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","\n","\n","            ###########################\n","            ### LassoLarsCV Imaging ###\n","            ###########################\n","\n","###            print(\"LASSOLARS_CV IMAGING\")\n","###            search_time_start_llarsi = time.time()\n","###            llars_cvi = LassoLarsCV(cv=cvreg_inner, max_iter=10000, n_jobs=4)\n","###            llars_cvi_score = []\n","\n","###            for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","###                llars_cvi.fit(X3.values[trainCV], y3[trainCV])\n","###                print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","###                      format(k, llars_cvi.alpha_, llars_cvi.score(X3.values[testCV], y3[testCV])))\n","###                llars_cvi_score.append(llars_cvi.score(X3.values[testCV], y3[testCV]))\n","###            search_time_stop_llarsi = time.time()\n","\n","###            feature_list_lli = []\n","###            for i in list(range(0,len(X3.columns))):\n","###                feature_list_lli.append([llars_cvi.coef_[i], X3.columns[i]])\n","###                df_featimp_lli = pd.DataFrame(data=feature_list_lli, columns=['coef','feature'])\n","###            df_featimp_lli.reindex(df_featimp_lli.coef.abs().sort_values(ascending=False).index)\n","###            df_featimp_sum_lli = df_featimp_lli[(df_featimp_lli['coef'] > .001) | (df_featimp_lli['coef'] < -.001)]\n","###            print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","###        else:\n","###            print(\"Imaging regularization not available for CLDRC dataset\")\n","\n","\n","###        print(\"LassoCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","###              format(np.mean(lasso_cvb_score), np.std(lasso_cvb_score), lasso_cvb.alpha_, search_time_stop_lassob-search_time_start_lassob))\n","###        if data == 3 or data ==4:\n","###            print(\"LassoCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","###                  format(np.mean(lasso_cvi_score), np.std(lasso_cvi_score), lasso_cvi.alpha_, search_time_stop_lassoi-search_time_start_lassoi))\n","        print(\"LLarsCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","              format(np.mean(llars_cvb_score), np.std(llars_cvb_score), llars_cvb.alpha_, search_time_stop_llarsb-search_time_start_llarsb))\n","###        if data == 3 or data ==4:\n","###            print(\"LLarsCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; time: {3:.3f}\".\n","###                  format(np.mean(llars_cvi_score), np.std(llars_cvi_score), llars_cvi.alpha_, search_time_stop_llarsi-search_time_start_llarsi))\n","\n","###        print('Lasso Behavior Weights')\n","###        print(df_featimp_sum_lassob.reindex(df_featimp_sum_lassob.coef.abs().sort_values(ascending=False).index))\n","###        print('')\n","###        if data == 3 or data ==4:\n","###            print('Lasso Imaging Weights')\n","###            print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","###            print('')\n","        print('LassoLars Behavior Weights')\n","        print(df_featimp_sum_llb.reindex(df_featimp_sum_llb.coef.abs().sort_values(ascending=False).index))\n","        print('')\n","###        if data == 3 or data ==4:\n","###            print('LassoLars Imaging Weights')\n","###            print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","###            print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yXfAobeG97hq"},"outputs":[],"source":["with open(\"0.results_demoonly_varloop_llars.txt\", 'w') as f:\n","    f.write(capture.stdout)"]},{"cell_type":"markdown","metadata":{"id":"k30fFq3I97hr"},"source":["# ESTIMATOR/SPECIFICITY, Dataset, Variable Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEfmJZr097hr"},"outputs":[],"source":["%%capture --no-stderr capture\n","\n","for j in range(1,5): # CURRENTLY JUST HUB IMPUTED!!! MODIFY to (1,5) for all 4 datasets\n","    for z in range(1,2): # READ\n","#    for z in range(2,3): # IN\n","#    for z in range(3,4): # HI\n","        for q in range(2,6):\n","            data = j\n","            variable = z\n","            estimator = q\n","            imaging = 7\n","            #####################\n","            # DATASET SELECTION #\n","            #####################\n","\n","            if data == 1 and variable == 1:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 2:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 3:\n","                dataset_name = \"CLDRC_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_cldrc.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 2 and variable == 1:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 2:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 2 and variable == 3:\n","                dataset_name = \"CLDRC_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_cldrc.csv\", index_col=0)\n","            elif data == 3 and variable == 1:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 2:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 3:\n","                dataset_name = \"Hub_DropNA\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_hub.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 4 and variable == 1:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 2:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_hub.csv\", index_col=0)\n","            elif data == 4 and variable == 3:\n","                dataset_name = \"Hub_Imputed\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_hub.csv\", index_col=0)\n","            else:\n","                print(\"Data and Variable Not Selected\")\n","\n","            #######################\n","            # IMPORT IMAGING DATA #\n","            #######################\n","\n","            if (data == 3 or data == 4):\n","                if imaging == 1:\n","                    df_i = pd.read_csv('1.fs5new.csv', na_values='NaN', index_col=0)\n","                    i_filename = \"fs5_labels\"\n","                elif imaging == 2:\n","                    df_i = pd.read_csv('1.tbssall.csv', na_values='NaN', index_col=0)                      # CAN REMOVE ALL FROM tbss\n","                    i_filename = \"tbss\"\n","                elif imaging == 3:\n","                    df_i = pd.read_csv('1.aseg.csv', na_values='NaN', index_col=0)\n","                    i_filename = \"aseg\"\n","                elif imaging == 4:\n","                    df_i = pd.read_csv('1.fs5new_tbssall.csv', na_values='NaN', index_col=0)                     # CAN REMOVE ALL FROM tbss\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_i = df_fs5.merge(df_tbss, on='scanid1')\n","                    i_filename = \"fs5_tbss\"\n","                elif imaging == 5:\n","                    df_i = pd.read_csv('1.fs5new_aseg.csv', na_values='NaN', index_col=0)\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","                    df_i = df_fs5.merge(df_aseg, on='scanid1')\n","                    i_filename = \"fs5_aseg\"\n","                elif imaging == 6:\n","                    df_i = pd.read_csv('1.tbssall_aseg.csv', na_values='NaN', index_col=0)                    # CAN REMOVE ALL FROM tbss\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","            #        df_i = df_tbss.merge(df_aseg, on='scanid1')\n","                    i_filename = \"tbss_aseg\"\n","                elif imaging == 7:\n","                    df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)                # CAN REMOVE ALL FROM tbss\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","            #        df_pre_i = df_fs5.merge(df_tbss, on='scanid1')\n","            #        df_i = df_pre_i.merge(df_aseg, on='scanid1')\n","                    i_filename = \"fs5_tbss_aseg\"\n","                else:\n","                    print('No imaging selected for Hub analysis')\n","            else:\n","                df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","                df_with_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","                print('No imaging in CLDRC')\n","\n","            ################################\n","            # COMBINE BEHAVIORAL & IMAGING #\n","            ################################\n","\n","            if data == 3 or data == 4:\n","                df_with_i = df.merge(df_i, on='scanid1')\n","                df_with_i = df_with_i.dropna()\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df.rename(columns={'Unnamed: 0_x':'Unnamed: 0'}, inplace=True)\n","            #    df_with_i.drop(df_with_i.columns[13], axis=1, inplace=True)\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df_with_i.reset_index(drop=True,inplace=True)\n","\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","#            print(\"Imaging File: \", i_filename)\n","#            print(\"Dataset Name: \", dataset_name)\n","#            print(\"Model Variable: \", model_var)\n","\n","            if 'Unnamed: 0' in df.columns:\n","                print('Unnamed: 0 in df')\n","            if 'Unnamed: 0' in df_i.columns:\n","                print('Unnamed: 0 in df_i')\n","            if 'Unnamed: 0' in df_with_i.columns:\n","                print('Unnamed: 0 in df_with_i')\n","\n","            ######################\n","            # SETTING ORIG_INDEX #\n","            ######################\n","\n","            df.reset_index(inplace=True)\n","            df_i.reset_index(inplace=True)\n","            df_with_i.reset_index(inplace=True)\n","\n","            df.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_i.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_with_i.rename(columns={'index':'orig_index'}, inplace=True)\n","\n","            #######################\n","            # ESTIMATOR SELECTION #\n","            #######################\n","\n","            if estimator == 1:\n","                model_estimator = \"Linear_SVR\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = LinearSVR(max_iter=10000)\n","            elif estimator == 2:\n","                model_estimator = \"SVR_Linear_Kernel\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                              \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'linear')\n","            elif estimator == 3:\n","                model_estimator = \"SVR_Nonlinear_RBF\"\n","                param_dist = {\"C\": uniform(.0001,5),\n","                             \"epsilon\": uniform(0,1)}\n","                alg = SVR(kernel = 'rbf', gamma = 'auto')\n","            elif estimator == 4:\n","                model_estimator=\"XGBoost\"\n","                param_dist = {\"n_estimators\": randint(5,100),\n","                              \"learning_rate\": uniform(0.01,0.5),\n","                              \"gamma\": uniform(0.0,0.5),\n","                              \"max_depth\": randint(3,10),\n","                              \"min_child_weight\": randint(1,8),\n","                              \"subsample\": uniform(.6,0.4),\n","                              \"colsample_bytree\": uniform(.3,0.7),\n","                              \"reg_lambda\": uniform(0.001,5),\n","                              \"reg_alpha\": uniform(0.00001,1)}\n","                alg = XGBRegressor(n_jobs=6)\n","            elif estimator == 5:\n","                model_estimator = \"Linear_Regression\"\n","                alg = linear_model.LinearRegression(n_jobs=8)\n","            else:\n","                print(\"Estimator Not Selected\")\n","\n","            print(\"Model Estimator: \", model_estimator)\n","\n","            #########################\n","            # DATASET PREPROCESSING #\n","            #########################\n","\n","            baselength = len(df.columns)\n","            base = [3,4]\n","            base2 = list(range(6,len(df.columns)))\n","            base.extend(base2)\n","            full1_preprocessor = make_column_transformer(\n","                (StandardScaler(), base),\n","                remainder='passthrough')\n","\n","            df.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","            if data == 3 or data == 4:\n","                df_with_i.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","\n","            i_index1 = index1 = df.columns.values[3]\n","            index2 = df.columns.values[6:].tolist()\n","            i_index3 = index3 = df.columns.values[5]\n","            i_index4 = index4 = df.columns.values[0:3].tolist()\n","            i_index5 = index5 = df.columns.values[4]\n","\n","            index2.extend(index4)\n","            index2.insert(0,index1)\n","            index2.insert(1,index5)\n","            index2.extend([index3])\n","\n","            df_scaled = full1_preprocessor.fit_transform(df)\n","            df_scaled = pd.DataFrame(data=df_scaled)\n","            df_scaled.columns = index2\n","\n","            if data == 3 or data == 4:\n","                base3 = list(range(6,len(df_i.columns)+baselength-2))\n","\n","                full = [3,4]\n","                full.extend(base3)\n","                full2_preprocessor = make_column_transformer(\n","                    (StandardScaler(), full),\n","                    remainder='passthrough')\n","\n","                i_index2 = df_with_i.columns.values[6:].tolist()\n","                i_index2.extend(i_index4)\n","                i_index2.insert(0,i_index1)\n","                i_index2.insert(1,i_index5)\n","                i_index2.extend([i_index3])\n","\n","                df_scaled_i = full2_preprocessor.fit_transform(df_with_i)\n","                df_scaled_i = pd.DataFrame(data=df_scaled_i)\n","                df_scaled_i.columns = i_index2\n","\n","            print(\"Data Preprocessed - Scaler Applied\")\n","\n","            ######################\n","            # FEATURES SELECTION #\n","            ######################\n","\n","            x1 = [1,2]\n","            x2 = [len(df_scaled.columns)-1]\n","            x3 = list(range(3,len(df_scaled.columns)-4))\n","            X1_vars = x1 + x2\n","            X2_vars = x1 + x2 + x3\n","\n","            if data == 3 or data ==4:\n","                x5 = [len(df_scaled_i.columns)-1]\n","                x4 = list(range(len(df_scaled.columns)-4,len(df_scaled_i.columns)-4))\n","                X3_vars = x1 + x5 + x3 + x4\n","\n","            X1 = df_scaled.iloc[:,X1_vars]\n","            y1 = df_scaled.iloc[:,0]\n","            if data == 1 or data == 2:\n","                groups1 = groups2 = df_scaled['famid']\n","            elif data == 3 or data == 4:\n","                groups1 = groups2 = df_scaled['fid']\n","            else:\n","                print('Dataset not set')\n","\n","            X2 = df_scaled.iloc[:,X2_vars]\n","            y2 = df_scaled.iloc[:,0]\n","            features_full1 = ['Age']\n","            for i in [X2_vars[1:]]:\n","                features_full1.extend(df_scaled.columns[i])\n","            features_demo = features_full1[0:3]\n","\n","            if data == 3 or data == 4:\n","                X3 = df_scaled_i.iloc[:,X3_vars]\n","                y3 = df_scaled_i.iloc[:,0]\n","                groups3 = df_scaled_i['fid'] # CHANGED FROM df_scaled.columns TO GET CORRECT # FAMID/GROUPS\n","                features_full2 = ['Age']\n","                for i in [X3_vars[1:]]:\n","                    features_full2.extend(df_scaled_i.columns[i])\n","\n","\n","            ##############################\n","            # CROSS-VALIDATION SELECTION #\n","            ##############################\n","\n","            inner_rstate = 9052017\n","            outer_rstate = 12301985\n","            #inner_rstate = np.random.RandomState(9052017)\n","            #outer_rstate = np.random.RandomState(12301985)\n","\n","            cv=GroupShuffleSplit\n","            #cv=GroupKFold\n","            cv_inner = cv(n_splits=20, test_size=0.2, random_state=inner_rstate)\n","            cv_outer = cv(n_splits=10, test_size=0.2, random_state=outer_rstate)\n","            n_iter=200\n","            n_iter_str = str(n_iter)\n","\n","            print(\"Features Selected\")\n","#            print(\"Inner Training CV: \", cv_inner)\n","#            print(\"Outer Testing CV: \", cv_outer)\n","\n","#            print(\"Dataset: \", dataset_name)\n","#            print(\"Model Variable: \", model_var)\n","#            print(\"Model Estimator: \", model_estimator)\n","#            print(\"Imaging File: \", i_filename)\n","\n","            ############################\n","            # SET SPECIFICITY OUTCOMES #\n","            ############################\n","\n","            if dataset_name == \"CLDRC_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=1265)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1265_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PARTIAL CLDRC CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=1190)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1190_specificity_outcomes.csv\")\n","            elif dataset_name == \"CLDRC_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL CLDRC CROSS-CONSTRUCT SPECIFICITY RESULTS (N=1675)\"\n","                df_outcomes = pd.read_csv(\"cldrc_1675_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and model_var == \"READ\":\n","                specificity_file = \"PARTIAL HUB CROSS-CONSTRUCT READ SPECIFICITY RESULTS (N=214)\"\n","                df_outcomes = pd.read_csv(\"hub_214_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_DropNA\" and (model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"PATIAL HUB CROSS-CONSTRUCT IN/HI SPECIFICITY RESULTS (N=209)\"\n","                df_outcomes = pd.read_csv(\"hub_209_specificity_outcomes.csv\")\n","            elif dataset_name == \"Hub_Imputed\" and (model_var == \"READ\" or model_var == \"IN\" or model_var == \"HI\"):\n","                specificity_file = \"FULL HUB CROSS-CONSTRUCT SPECIFICITY RESULTS (N=324)\"\n","                df_outcomes = pd.read_csv(\"hub_324_specificity_outcomes.csv\")\n","            else:\n","                print(\"Dataset and Model Variables Not Set\")\n","\n","#            print(\"\")\n","            print(\"Specificity Filename: \", specificity_file)\n","            print(\"\")\n","            r2 = df_outcomes['out_read_fs']\n","            i2 = df_outcomes['out_in_fs']\n","            h2 = df_outcomes['out_hi_fs']\n","            m2 = df_outcomes['out_math_fs']\n","\n","            #########################################\n","            ### RESULTS WITH DEMOGRAPHIC FEATURES ###\n","            #########################################\n","\n","#            print(\"Dataset: \", dataset_name)\n","#            print(\"Model Variable: \", model_var)\n","#            print(\"Model Estimator: \", model_estimator)\n","#            print(\"\")\n","\n","#            print(\"Model Tuning Times: \\n\")\n","\n","            for train1, test1 in cv_inner.split(X1, y1, groups=groups1):\n","                X_train1, X_test1 = X1.values[train1], X1.values[test1]\n","                y_train1, y_test1 = y1[train1], y1[test1]\n","                groups_train1, groups_test1 = groups1[train1], groups1[test1]\n","\n","            search_time_start1 = time.time()\n","#            print(\"Demographics tuning time...\")\n","            if estimator == 5:\n","                clf1 = alg\n","            else:\n","                clf1 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf1.fit(X_train1, y_train1, groups = groups_train1)\n","\n","            search_time_stop1 = time.time()\n","#            print('%.3f' %(search_time_stop1 - search_time_start1))\n","#            print(\"\")\n","\n","            if estimator == 5:\n","                cvalscorer1 = clf1\n","            else:\n","                cvalscorer1 = clf1.best_estimator_\n","\n","            train1_scores = cross_val_score(cvalscorer1, X_train1, y_train1, groups=groups_train1, cv=cv_inner, n_jobs=4)\n","            test1_scores = cross_val_score(cvalscorer1, X_test1, y_test1, groups=groups_test1, cv=cv_outer, n_jobs=4)\n","\n","            #################################\n","            ### RESULTS WITH ALL FEATURES ###\n","            #################################\n","\n","            for train2, test2 in cv_inner.split(X2, y2, groups=groups2):\n","                X_train2, X_test2 = X2.values[train2], X2.values[test2]\n","                y_train2, y_test2 = y2[train2], y2[test2]\n","                groups_train2, groups_test2 = groups2[train2], groups2[test2]\n","                r_train2, r_test2 = r2[train2], r2[test2]\n","                i_train2, i_test2 = i2[train2], i2[test2]\n","                h_train2, h_test2 = h2[train2], h2[test2]\n","                m_train2, m_test2 = m2[train2], m2[test2]\n","\n","            search_time_start2 = time.time()\n","#            print(\"Full Model tuning time...\")\n","            if estimator == 5:\n","                clf2 = alg\n","            else:\n","                clf2 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf2.fit(X_train2, y_train2, groups = groups_train2)\n","\n","            search_time_stop2 = time.time()\n","#            print('%.3f' %(search_time_stop2 - search_time_start2))\n","#            print(\"\\n\"*2)\n","\n","            if estimator == 5:\n","                cvalscorer2 = clf2\n","            else:\n","                cvalscorer2 = clf2.best_estimator_\n","\n","            train2_scores = cross_val_score(cvalscorer2, X_train2, y_train2, groups=groups_train2, cv=cv_inner, n_jobs=4)\n","            test2_scores = cross_val_score(cvalscorer2, X_test2, y_test2, groups=groups_test2, cv=cv_outer, n_jobs=4)\n","\n","#            print(\"DEMOGRAPHIC MODEL RESULTS\")\n","#            print(\"\")\n","#            print(\"Train [inner CV] R2s:\\n\", train1_scores)\n","#            print(\"Train R2s Mean:\\n%.3f\" %train1_scores.mean())\n","#            print(\"Train R2s SD:\\n%.3f\" %train1_scores.std())\n","#            print(\"\")\n","#            print(\"Test [outer CV] R2s:\\n\", test1_scores)\n","#            print(\"Test R2s Mean:\\n%.3f\" %test1_scores.mean())\n","#            print(\"Test R2s SD:\\n%.3f\" %test1_scores.std())\n","#            print(\"\")\n","            if estimator != 5:\n","                print(\"Best Demo Estimator:\\n\", clf1.best_estimator_)\n","                print(\"\")\n","                print(\"Best Demo Model params:\")\n","                best_params1 = clf1.best_params_\n","                for param_name in sorted(best_params1.keys()):\n","                    print('%s: %r' % (param_name, best_params1[param_name]))\n","#            print(\"\\n\"*2)\n","#            print(\"FULL BEHAVIORAL MODEL RESULTS\")\n","#            print(\"\")\n","#            print(\"Train [inner CV] R2s:\\n\", train2_scores)\n","#            print(\"Train R2s Mean:\\n%.3f\" %train2_scores.mean())\n","#            print(\"Train R2s SD:\\n%.3f\" %train2_scores.std())\n","#            print(\"\")\n","#            print(\"Test [outer CV] R2s:\\n\", test2_scores)\n","#            print(\"Test R2s Mean:\\n%.3f\" %test2_scores.mean())\n","#            print(\"Test R2s SD:\\n%.3f\" %test2_scores.std())\n","#            print(\"\")\n","            if estimator != 5:\n","                print(\"Best Full Estimator:\\n\", clf2.best_estimator_)\n","                print(\"\")\n","                print(\"Best Full params:\")\n","                best_params2 = clf2.best_params_\n","                for param_name in sorted(best_params2.keys()):\n","                    print('%s: %r' % (param_name, best_params2[param_name]))\n","\n","            ###########################################\n","            ### RESULTS WITH ALL FEATURES & IMAGING ###\n","            ###########################################\n","\n","            if data == 3 or data == 4:\n","#                print(\"Imaging File: \", i_filename)\n","#                print(\"\")\n","\n","                for train3, test3 in cv_inner.split(X3, y3, groups=groups3):\n","                    X_train3, X_test3 = X3.values[train3], X3.values[test3]\n","                    y_train3, y_test3 = y3[train3], y3[test3]\n","                    groups_train3, groups_test3 = groups3[train3], groups3[test3]\n","                    r_train3, r_test3 = r2[train3], r2[test3]\n","                    i_train3, i_test3 = i2[train3], i2[test3]\n","                    h_train3, h_test3 = h2[train3], h2[test3]\n","                    m_train3, m_test3 = m2[train3], m2[test3]\n","\n","#                print(\"Full Model & Imaging tuning time...\")\n","                search_time_start3 = time.time()\n","                if estimator == 5:\n","                    clf3 = alg\n","                else:\n","                    clf3 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                    clf3.fit(X_train3, y_train3, groups = groups_train3)\n","                search_time_stop3 = time.time()\n","#                print('%.3f' %(search_time_stop3 - search_time_start3))\n","#                print(\"\\n\"*3)\n","\n","                if estimator == 5:\n","                    cvalscorer3 = clf3\n","                else:\n","                    cvalscorer3 = clf3.best_estimator_\n","\n","                train3_scores = cross_val_score(cvalscorer3, X_train3, y_train3, groups=groups_train3, cv=cv_inner, n_jobs=4)\n","                test3_scores = cross_val_score(cvalscorer3, X_test3, y_test3, groups=groups_test3, cv=cv_outer, n_jobs=4)\n","\n","#                print(\"FULL BEHAVIORAL & IMAGING MODEL RESULTS\")\n","#                print(\"\")\n","#                print(\"Train [inner CV] R2s:\\n\", train3_scores)\n","#                print(\"Train R2s Mean:\\n%.3f\" %train3_scores.mean())\n","#                print(\"Train R2s SD:\\n%.3f\" %train3_scores.std())\n","#                print(\"\")\n","#                print(\"Test [outer CV] R2s:\\n\", test3_scores)\n","#                print(\"Test R2s Mean:\\n%.3f\" %test3_scores.mean())\n","#                print(\"Test R2s SD:\\n%.3f\" %test3_scores.std())\n","#                print(\"\")\n","                if estimator != 5:\n","                    print(\"Best Full Estimator:\\n\", clf3.best_estimator_)\n","                    print(\"\")\n","                    print(\"Best Full params:\")\n","                    best_params3 = clf3.best_params_\n","                    for param_name in sorted(best_params3.keys()):\n","                        print('%s: %r' % (param_name, best_params3[param_name]))\n","            else:\n","                print('No Imaging Data in CLDRC')\n","\n","            #########################################\n","            ####  DISCRIMINANT VALIDITY RESULTS  ####\n","            #########################################\n","\n","#            print(\"SPECIFICITY RESULTS - TEST SET\")\n","#            print(\"\")\n","#            print(\"Dataset: \", dataset_name)\n","#            print(\"Model Variable: \", model_var)\n","#            print(\"Model Estimator: \", model_estimator)\n","\n","#            print(\"\\n\"*2)\n","\n","            if estimator == 5:\n","                fullsample_scores_test = cross_val_score(clf2, X_test2, y_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_read_scores_test = cross_val_score(clf2, X_test2, r_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_in_scores_test = cross_val_score(clf2, X_test2, i_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_hi_scores_test = cross_val_score(clf2, X_test2, h_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_math_scores_test = cross_val_score(clf2, X_test2, m_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","            else:\n","                fullsample_scores_test = cross_val_score(clf2.best_estimator_, X_test2, y_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_read_scores_test = cross_val_score(clf2.best_estimator_, X_test2, r_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_in_scores_test = cross_val_score(clf2.best_estimator_, X_test2, i_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_hi_scores_test = cross_val_score(clf2.best_estimator_, X_test2, h_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","                var_math_scores_test = cross_val_score(clf2.best_estimator_, X_test2, m_test2, groups=groups_test2, cv=cv_outer, n_jobs=8)\n","\n","#            print(\"SPECIFICITY CORRELATIONS:\")\n","#            print(\"\")\n","#            print(model_var + \"-READ Test R Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_test.mean())))\n","#            print(model_var + \"-IN Test R Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_test.mean())))\n","#            print(model_var + \"-HI Test R Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_test.mean())))\n","#            print(model_var + \"-MATH Test R Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_test.mean())))\n","#            print(\"\\n\"*2)\n","\n","#            print(\"VARIANCES EXPLAINED:\")\n","#            print(\"\")\n","#            print(model_var + \"-\" + model_var + \" Test R2s Mean:\\n%.3f\" %fullsample_scores_test.mean())\n","#            print(model_var + \"-\" + model_var + \" Test R2s SD:\\n%.3f\" %fullsample_scores_test.std())\n","#            print(\"\")\n","            #print(model_var + \"-READ R2s:\\n\", var_read_scores)\n","#            print(model_var + \"-READ Test R2s Mean:\\n%.3f\" %var_read_scores_test.mean())\n","#            print(model_var + \"-READ Test R2s SD:\\n%.3f\" %var_read_scores_test.std())\n","#            print(\"\")\n","            #print(model_var + \"-IN R2s:\\n\", var_in_scores)\n","#            print(model_var + \"-IN Test R2s Mean:\\n%.3f\" %var_in_scores_test.mean())\n","#            print(model_var + \"-IN Test R2s SD:\\n%.3f\" %var_in_scores_test.std())\n","#            print(\"\")\n","            #print(model_var + \"-HI R2s:\\n\", var_hi_scores)\n","#            print(model_var + \"-HI Test R2s Mean:\\n%.3f\" %var_hi_scores_test.mean())\n","#            print(model_var + \"-HI Test R2s SD:\\n%.3f\" %var_hi_scores_test.std())\n","#            print(\"\")\n","            #print(model_var + \"-MATH R2s:\\n\", var_math_scores)\n","#            print(model_var + \"-MATH Test R2s Mean:\\n%.3f\" %var_math_scores_test.mean())\n","#            print(model_var + \"-MATH Test R2s SD:\\n%.3f\" %var_math_scores_test.std())\n","\n","            # Results of Training sample specificity:\n","\n","#            print(\"SPECIFICITY RESULTS - TRAIN SET\")\n","#            print(\"Dataset: \", dataset_name)\n","#            print(\"\")\n","\n","            if estimator == 5:\n","                fullsample_scores_train = cross_val_score(clf2, X_train2, y_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_read_scores_train = cross_val_score(clf2, X_train2, r_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_in_scores_train = cross_val_score(clf2, X_train2, i_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_hi_scores_train = cross_val_score(clf2, X_train2, h_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_math_scores_train = cross_val_score(clf2, X_train2, m_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","            else:\n","                fullsample_scores_train = cross_val_score(clf2.best_estimator_, X_train2, y_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_read_scores_train = cross_val_score(clf2.best_estimator_, X_train2, r_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_in_scores_train = cross_val_score(clf2.best_estimator_, X_train2, i_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_hi_scores_train = cross_val_score(clf2.best_estimator_, X_train2, h_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","                var_math_scores_train = cross_val_score(clf2.best_estimator_, X_train2, m_train2, groups=groups_train2, cv=cv_outer, n_jobs=8)\n","\n","#            print(model_var + \"-\" + model_var + \" Train R Mean:\\n%.3f\" %math.sqrt(abs(fullsample_scores_train.mean())))\n","#            print(model_var + \"-READ Train R Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_train.mean())))\n","#            print(model_var + \"-IN Train R Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_train.mean())))\n","#            print(model_var + \"-HI Train R Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_train.mean())))\n","#            print(model_var + \"-MATH Train R Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_train.mean())))\n","\n","            # Results of FULL sample specificity:\n","            if data == 2 or data == 4:\n","                if estimator == 5:\n","                    fullsample_scores_full = cross_val_score(clf2, X2, y2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_read_scores_full = cross_val_score(clf2, X2, r2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_in_scores_full = cross_val_score(clf2, X2, i2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_hi_scores_full = cross_val_score(clf2, X2, h2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_math_scores_full = cross_val_score(clf2, X2, m2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                else:\n","                    fullsample_scores_full = cross_val_score(clf2.best_estimator_, X2, y2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_read_scores_full = cross_val_score(clf2.best_estimator_, X2, r2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_in_scores_full = cross_val_score(clf2.best_estimator_, X2, i2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_hi_scores_full = cross_val_score(clf2.best_estimator_, X2, h2, groups=groups2, cv=cv_outer, n_jobs=8)\n","                    var_math_scores_full = cross_val_score(clf2.best_estimator_, X2, m2, groups=groups2, cv=cv_outer, n_jobs=8)\n","#                print(\"SPECIFICITY RESULTS - TEST + TRAIN SETS\")\n","#                print(\"Dataset: \", dataset_name)\n","#                print(\"\")\n","\n","#                print(model_var + \"-\" + model_var + \" Full Sample R Mean:\\n%.3f\" %math.sqrt(abs(fullsample_scores_full.mean())))\n","#                print(model_var + \"-READ Full Sample R Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_full.mean())))\n","#                print(model_var + \"-IN Full Sample R Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_full.mean())))\n","#                print(model_var + \"-HI Full Sample R Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_full.mean())))\n","#                print(model_var + \"-MATH Full Sample R Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_full.mean())))\n","            else:\n","                print(\"Dataset is not imputed for estimation of full-sample correlations\")\n","\n","            #################################\n","            ###   FULL CONTENT TO PRINT   ###\n","            #################################\n","\n","            print(\"Dataset_Name: \", dataset_name)\n","            print(\"Model_Variable: \", model_var)\n","            print(\"Model_Estimator: \", model_estimator)\n","            print(\"Imaging_File: \", i_filename)\n","            print(\"\")\n","            print(\"Data_Preprocessed_-_Scaler_Applied\")\n","            print(\"Features_Selected\")\n","            print(\"Inner_Training_CV: \", cv_inner)\n","            print(\"Outer_Testing_CV: \", cv_outer)\n","            print(\"Specificity_Filename: \", specificity_file)\n","            print(\"\")\n","\n","            print(\"Model_Tuning_Times: \\n\")\n","            print(\"Demographics_tuning_time: \")\n","            print(\"%.3f\" %(search_time_stop1 - search_time_start1))\n","            print(\"\")\n","            print(\"Full_Model_tuning_time: \")\n","            print(\"%.3f\" %(search_time_stop2 - search_time_start2))\n","            print(\"\")\n","\n","            if data == 3 or data == 4:\n","                print(\"Full_Model_&_Imaging_tuning_time: \")\n","                print(\"%.3f\" %(search_time_stop3 - search_time_start3))\n","            print(\"\")\n","\n","            print(\"DEMOGRAPHIC_MODEL\")\n","            print(\"\")\n","            if estimator == 5:\n","                print(\"No Best Estimator for LinearRegression\")\n","            else:\n","                print(\"Demo_Best_Estimator:\\n\", clf1.best_estimator_)\n","            print(\"\")\n","            print(\"Demo_Train_R2s_Mean:\\n%.3f\" %train1_scores.mean())\n","            print(\"Demo_Train_R2s_SD:\\n%.3f\" %train1_scores.std())\n","            print(\"\")\n","            print(\"Demo_Test_R2s_Mean:\\n%.3f\" %test1_scores.mean())\n","            print(\"Demo_Test_R2s_SD:\\n%.3f\" %test1_scores.std())\n","            print(\"\")\n","\n","            print(\"FULL_MODEL\")\n","            print(\"\")\n","            if estimator == 5:\n","                print(\"No Best Estimator for LinearRegression\")\n","            else:\n","                print(\"Full_Best_Estimator:\\n\", clf2.best_estimator_)\n","            print(\"\")\n","            print(\"Full_Train_R2s_Mean:\\n%.3f\" %train2_scores.mean())\n","            print(\"Full_Train_R2s_SD:\\n%.3f\" %train2_scores.std())\n","            print(\"\")\n","            print(\"Full_Test_R2s_Mean:\\n%.3f\" %test2_scores.mean())\n","            print(\"Full_Test_R2s_SD:\\n%.3f\" %test2_scores.std())\n","            print(\"\")\n","\n","            if data == 3 or data == 4:\n","                print(\"FULL_&_IMAGING_MODEL\")\n","                print(\"\")\n","                if estimator == 5:\n","                    print(\"No Best Estimator for LinearRegression\")\n","                else:\n","                    print(\"Imaging_Best_Estimator:\\n\", clf3.best_estimator_)\n","                print(\"\")\n","                print(\"Imaging_Train_R2s_Mean:\\n%.3f\" %train3_scores.mean())\n","                print(\"Imaging_Train_R2s_SD:\\n%.3f\" %train3_scores.std())\n","                print(\"\")\n","                print(\"Imaging_Test_R2s_Mean:\\n%.3f\" %test3_scores.mean())\n","                print(\"Imaging_Test_R2s_SD:\\n%.3f\" %test3_scores.std())\n","                print(\"\")\n","\n","            print(\"SPECIFICITY_CORRELATIONS_-_TEST_SET\")\n","            print(\"\")\n","            print(model_var + \"-READ_Test_R_Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_test.mean())))\n","            print(model_var + \"-IN_Test_R_Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_test.mean())))\n","            print(model_var + \"-HI_Test_R_Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_test.mean())))\n","            print(model_var + \"-MATH_Test_R_Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_test.mean())))\n","            print(\"\")\n","            print(\"SPECIFICITY_CORRELATIONS_-_TRAIN_SET\")\n","            print(\"\")\n","            print(model_var + \"-READ_Train_R_Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_train.mean())))\n","            print(model_var + \"-IN_Train_R_Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_train.mean())))\n","            print(model_var + \"-HI_Train_R_Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_train.mean())))\n","            print(model_var + \"-MATH_Train_R_Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_train.mean())))\n","            print(\"\")\n","\n","            if data == 2 or data == 4:\n","                print(\"SPECIFICITY_CORRELATIONS_-_TEST_+_TRAIN_SETS\")\n","                print(model_var + \"-READ_Full_Sample_R_Mean:\\n%.3f\" %math.sqrt(abs(var_read_scores_full.mean())))\n","                print(model_var + \"-IN_Full_Sample_R_Mean:\\n%.3f\" %math.sqrt(abs(var_in_scores_full.mean())))\n","                print(model_var + \"-HI_Full_Sample_R_Mean:\\n%.3f\" %math.sqrt(abs(var_hi_scores_full.mean())))\n","                print(model_var + \"-MATH_Full_Sample_R_Mean:\\n%.3f\" %math.sqrt(abs(var_math_scores_full.mean())))\n","                print(\"\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1TksPiMQ97hs"},"outputs":[],"source":["with open(\"0.results_hubvarloop_estimators_specificity_READ.txt\", 'w') as f:\n","    f.write(capture.stdout)\n","\n","#with open(\"0.results_hubvarloop_estimators_specificity_IN.txt\", 'w') as f:\n","#    f.write(capture.stdout)\n","\n","#with open(\"0.results_hubvarloop_estimators_specificity_HI.txt\", 'w') as f:\n","#    f.write(capture.stdout)"]},{"cell_type":"markdown","metadata":{"id":"_AIGVQ9B97ht"},"source":["# DIAGNOSTIC ACCURACY Estimator (Data/Var) Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0yHBfAc97ht"},"outputs":[],"source":["%%capture --no-stderr capture\n","\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.svm import SVC, LinearSVC\n","from xgboost import XGBClassifier\n","from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report,  f1_score\n","\n","#for j in range(1,5): # MODIFY to (1,5) for all 4 datasets\n","for j in [1,2,4]: # ALL DATA EXCEPT HUBDROPNA\n","#    for z in range(1,2): # READ\n","#    for z in range(2,3): # IN\n","    for z in range(3,4): # HI\n","#    for z in range(4,5): # ADHD\n","#    for z in range(5,6): # ADHD TYPE\n","        for q in range(2,6):\n","            data = j\n","            variable = z\n","            estimator = q\n","            imaging = 7\n","\n","            #####################\n","            # DATASET SELECTION #\n","            #####################\n","\n","            if data == 1 and variable == 1:\n","                dataset_name = \"CLDRC_DropNA_dx\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_cldrc_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 2:\n","                dataset_name = \"CLDRC_DropNA_dx\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_cldrc_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 3:\n","                dataset_name = \"CLDRC_DropNA_dx\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_cldrc_dx.csv\", na_values = 'NaN', index_col=0)\n","\n","            elif data == 1 and variable == 4:\n","                dataset_name = \"CLDRC_DropNA_dx\"\n","                model_var = \"ADHD\"\n","                df = pd.read_csv(\"df_vars_adhdfs_dropna_cldrc_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 1 and variable == 5:\n","                dataset_name = \"CLDRC_DropNA_dx\"\n","                model_var = \"ADHD TYPE\"\n","                df = pd.read_csv(\"df_vars_adhdtype_dropna_cldrc_dx.csv\", na_values = 'NaN', index_col=0)\n","\n","\n","            elif data == 2 and variable == 1:\n","                dataset_name = \"CLDRC_Imputed_dx\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_cldrc_dx.csv\", index_col=0)\n","            elif data == 2 and variable == 2:\n","                dataset_name = \"CLDRC_Imputed_dx\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_cldrc_dx.csv\", index_col=0)\n","            elif data == 2 and variable == 3:\n","                dataset_name = \"CLDRC_Imputed_dx\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_cldrc_dx.csv\", index_col=0)\n","\n","            elif data == 2 and variable == 4:\n","                dataset_name = \"CLDRC_Imputed_dx\"\n","                model_var = \"ADHD\"\n","                df = pd.read_csv(\"df_vars_adhdfs_imputed_cldrc_dx.csv\", index_col=0)\n","            elif data == 2 and variable == 5:\n","                dataset_name = \"CLDRC_Imputed_dx\"\n","                model_var = \"ADHD TYPE\"\n","                df = pd.read_csv(\"df_vars_adhdtype_imputed_cldrc_dx.csv\", index_col=0)\n","\n","\n","            elif data == 3 and variable == 1:\n","                dataset_name = \"Hub_DropNA_dx\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_dropna_hub_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 2:\n","                dataset_name = \"Hub_DropNA_dx\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_dropna_hub_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 3:\n","                dataset_name = \"Hub_DropNA_dx\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_dropna_hub_dx.csv\", na_values = 'NaN', index_col=0)\n","\n","            elif data == 3 and variable == 4:\n","                dataset_name = \"Hub_DropNA_dx\"\n","                model_var = \"ADHD\"\n","                df = pd.read_csv(\"df_vars_adhdfs_dropna_hub_dx.csv\", na_values = 'NaN', index_col=0)\n","            elif data == 3 and variable == 5:\n","                dataset_name = \"Hub_DropNA_dx\"\n","                model_var = \"ADHD TYPE\"\n","                df = pd.read_csv(\"df_vars_adhdtype_dropna_hub_dx.csv\", na_values = 'NaN', index_col=0)\n","\n","\n","            elif data == 4 and variable == 1:\n","                dataset_name = \"Hub_Imputed_dx\"\n","                model_var = \"READ\"\n","                df = pd.read_csv(\"df_vars_readfs_imputed_hub_dx.csv\", index_col=0)\n","            elif data == 4 and variable == 2:\n","                dataset_name = \"Hub_Imputed_dx\"\n","                model_var = \"IN\"\n","                df = pd.read_csv(\"df_vars_infs_imputed_hub_dx.csv\", index_col=0)\n","            elif data == 4 and variable == 3:\n","                dataset_name = \"Hub_Imputed_dx\"\n","                model_var = \"HI\"\n","                df = pd.read_csv(\"df_vars_hifs_imputed_hub_dx.csv\", index_col=0)\n","\n","            elif data == 4 and variable == 4:\n","                dataset_name = \"Hub_Imputed_dx\"\n","                model_var = \"ADHD\"\n","                df = pd.read_csv(\"df_vars_adhdfs_imputed_hub_dx.csv\", index_col=0)\n","            elif data == 4 and variable == 5:\n","                dataset_name = \"Hub_Imputed_dx\"\n","                model_var = \"ADHD TYPE\"\n","                df = pd.read_csv(\"df_vars_adhdtype_imputed_hub_dx.csv\", index_col=0)\n","\n","\n","            else:\n","                print(\"Data and Variable Not Selected\")\n","\n","            #######################\n","            # IMPORT IMAGING DATA #\n","            #######################\n","\n","            if (data == 3 or data == 4):\n","                if imaging == 1:\n","                    df_i = pd.read_csv('1.fs5new.csv', na_values='NaN', index_col=0)\n","                    i_filename = \"fs5_labels\"\n","                elif imaging == 2:\n","                    df_i = pd.read_csv('1.tbssall.csv', na_values='NaN', index_col=0)                      # CAN REMOVE ALL FROM tbss\n","                    i_filename = \"tbss\"\n","                elif imaging == 3:\n","                    df_i = pd.read_csv('1.aseg.csv', na_values='NaN', index_col=0)\n","                    i_filename = \"aseg\"\n","                elif imaging == 4:\n","                    df_i = pd.read_csv('1.fs5new_tbssall.csv', na_values='NaN', index_col=0)                     # CAN REMOVE ALL FROM tbss\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_i = df_fs5.merge(df_tbss, on='scanid1')\n","                    i_filename = \"fs5_tbss\"\n","                elif imaging == 5:\n","                    df_i = pd.read_csv('1.fs5new_aseg.csv', na_values='NaN', index_col=0)\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","                    df_i = df_fs5.merge(df_aseg, on='scanid1')\n","                    i_filename = \"fs5_aseg\"\n","                elif imaging == 6:\n","                    df_i = pd.read_csv('1.tbssall_aseg.csv', na_values='NaN', index_col=0)                    # CAN REMOVE ALL FROM tbss\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","            #        df_i = df_tbss.merge(df_aseg, on='scanid1')\n","                    i_filename = \"tbss_aseg\"\n","                elif imaging == 7:\n","                    df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)                # CAN REMOVE ALL FROM tbss\n","            #        df_fs5 = pd.read_csv('fs5_labels_NaN_reduced.csv', na_values='NaN', index_col=0)\n","            #        df_tbss = pd.read_csv('tbss_metrics_NaN_FA.csv', na_values='NaN', index_col=0)\n","            #        df_aseg = pd.read_csv('DKTaseg_hub.csv', na_values='NaN', index_col=0)\n","            #        df_pre_i = df_fs5.merge(df_tbss, on='scanid1')\n","            #        df_i = df_pre_i.merge(df_aseg, on='scanid1')\n","                    i_filename = \"fs5_tbss_aseg\"\n","                else:\n","                    print('No imaging selected for Hub analysis')\n","            else:\n","                df_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","                df_with_i = pd.read_csv('1.fs5new_tbssall_aseg.csv', na_values='NaN', index_col=0)\n","                print('No imaging in CLDRC')\n","\n","            ################################\n","            # COMBINE BEHAVIORAL & IMAGING #\n","            ################################\n","\n","            if data == 3 or data == 4:\n","                df_with_i = df.merge(df_i, on='scanid1')\n","                df_with_i = df_with_i.dropna()\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df.rename(columns={'Unnamed: 0_x':'Unnamed: 0'}, inplace=True)\n","            #    df_with_i.drop(df_with_i.columns[13], axis=1, inplace=True)\n","            #    df_with_i.drop('Unnamed: 0_y', axis=1, inplace=True)\n","            #    df_with_i.reset_index(drop=True,inplace=True)\n","\n","            else:\n","                i_filename = 'No_Imaging_in_Model'\n","#            print(\"Imaging File: \", i_filename)\n","#            print(\"Dataset Name: \", dataset_name)\n","#            print(\"Model Variable: \", model_var)\n","\n","            if 'Unnamed: 0' in df.columns:\n","                print('Unnamed: 0 in df')\n","            if 'Unnamed: 0' in df_i.columns:\n","                print('Unnamed: 0 in df_i')\n","            if 'Unnamed: 0' in df_with_i.columns:\n","                print('Unnamed: 0 in df_with_i')\n","\n","            ######################\n","            # SETTING ORIG_INDEX #\n","            ######################\n","\n","            df.reset_index(inplace=True)\n","            df_i.reset_index(inplace=True)\n","            df_with_i.reset_index(inplace=True)\n","\n","            df.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_i.rename(columns={'index':'orig_index'}, inplace=True)\n","            df_with_i.rename(columns={'index':'orig_index'}, inplace=True)\n","\n","            #######################\n","            # ESTIMATOR SELECTION #\n","            #######################\n","\n","            if estimator == 1:\n","                model_estimator = \"Linear_SVC\"\n","                param_dist = {\"C\": uniform(.0001,5)}\n","                alg = LinearSVC(penalty='l2', max_iter=10000, loss='squared_hinge') ### CHANGE l2 for l1 OR elasticnet ### CHANGE hinge for squared_hinge\n","            elif estimator == 2:\n","                model_estimator = \"SVC_Linear_Kernel\"\n","                param_dist = {\"C\": uniform(.0001,5)}\n","                alg = SVC(kernel = 'linear', class_weight='balanced')\n","            elif estimator == 3:\n","                model_estimator = \"SVC_Nonlinear_RBF\"\n","                param_dist = {\"C\": uniform(.0001,5)}\n","                alg = SVC(kernel = 'rbf', class_weight='balanced', gamma = 'auto')\n","            elif estimator == 4:\n","                model_estimator=\"XGBoost\"\n","                param_dist = {\"n_estimators\": randint(5,100),\n","                              \"learning_rate\": uniform(0.01,0.5),\n","                              \"gamma\": uniform(0.0,0.5),\n","                              \"max_depth\": randint(3,10),\n","                              \"min_child_weight\": randint(1,8),\n","                              \"subsample\": uniform(.6,0.4),\n","                              \"colsample_bytree\": uniform(.3,0.7),\n","                              \"reg_lambda\": uniform(0.001,5),\n","                              \"reg_alpha\": uniform(0.00001,1)}\n","                alg = XGBClassifier(n_jobs=6)\n","            elif estimator == 5:\n","                model_estimator = \"Logistic_Regression\"\n","                alg = linear_model.LogisticRegression(penalty='l1', max_iter=10000, solver='liblinear', class_weight='balanced') ### CHANGE l2 for l1 OR elasticnet (set l1_ratio) ###\n","                param_dist = {\"C\": uniform(.0001,5)}#,\n","                              #\"l1_ratio\": uniform(0,1)}\n","                #MAY NEED multi_class='multinomial' and solver='lbfgs'\n","\n","            else:\n","                print(\"Estimator Not Selected\")\n","\n","            print(\"Model Estimator: \", model_estimator)\n","\n","            #########################\n","            # DATASET PREPROCESSING #\n","            #########################\n","\n","            baselength = len(df.columns)\n","            base = [3,4]\n","            base2 = list(range(6,len(df.columns)))\n","            base.extend(base2)\n","            full1_preprocessor = make_column_transformer(\n","                (StandardScaler(), base),\n","                remainder='passthrough')\n","\n","            df.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","            if data == 3 or data == 4:\n","                df_with_i.rename(columns={'Unnamed: 0':'orig_index'}, inplace=True)\n","            ###################################\n","            ### POSSIBLY SHOULD BE EXCLUDED ###\n","            ###################################\n","\n","            i_index1 = index1 = df.columns.values[3]\n","            index2 = df.columns.values[6:].tolist()\n","            i_index3 = index3 = df.columns.values[5]\n","            i_index4 = index4 = df.columns.values[0:3].tolist()\n","            i_index5 = index5 = df.columns.values[4]\n","\n","            index2.extend(index4)\n","            index2.insert(0,index1)\n","            index2.insert(1,index5)\n","            index2.extend([index3])\n","\n","            df_scaled = full1_preprocessor.fit_transform(df)\n","            df_scaled = pd.DataFrame(data=df_scaled)\n","            df_scaled.columns = index2\n","\n","            if data == 3 or data == 4:\n","                base3 = list(range(6,len(df_i.columns)+baselength-2))\n","\n","                full = [3,4]\n","                full.extend(base3)\n","                full2_preprocessor = make_column_transformer(\n","                    (StandardScaler(), full),\n","                    remainder='passthrough')\n","\n","                i_index2 = df_with_i.columns.values[6:].tolist()\n","                i_index2.extend(i_index4)\n","                i_index2.insert(0,i_index1)\n","                i_index2.insert(1,i_index5)\n","                i_index2.extend([i_index3])\n","\n","                df_scaled_i = full2_preprocessor.fit_transform(df_with_i)\n","                df_scaled_i = pd.DataFrame(data=df_scaled_i)\n","                df_scaled_i.columns = i_index2\n","\n","            print(\"Data Preprocessed - Scaler Applied\")\n","\n","            ######################\n","            # FEATURES SELECTION #\n","            ######################\n","\n","            x1 = [1,2]\n","            x2 = [len(df_scaled.columns)-1]\n","            x3 = list(range(3,len(df_scaled.columns)-4))\n","            X1_vars = x1 + x2\n","            X2_vars = x1 + x2 + x3\n","\n","            if data == 3 or data ==4:\n","                x5 = [len(df_scaled_i.columns)-1]\n","                x4 = list(range(len(df_scaled.columns)-4,len(df_scaled_i.columns)-4))\n","                X3_vars = x1 + x5 + x3 + x4\n","\n","            X1 = df_scaled.iloc[:,X1_vars]\n","            y1 = df.iloc[:,3]                           # CHANGED FROM y1 = df_scaled.iloc[:,0] for categorical\n","            if data == 1 or data == 2:\n","                groups1 = groups2 = df_scaled['famid']\n","            elif data == 3 or data == 4:\n","                groups1 = groups2 = df_scaled['fid']\n","            else:\n","                print('Dataset not set')\n","\n","            X2 = df_scaled.iloc[:,X2_vars]\n","            y2 = df.iloc[:,3]                           # CHANGED FROM y1 = df_scaled.iloc[:,0] for categorical\n","            features_full1 = ['Age']\n","            for i in [X2_vars[1:]]:\n","                features_full1.extend(df_scaled.columns[i])\n","            features_demo = features_full1[0:3]\n","\n","            if data == 3 or data == 4:\n","                X3 = df_scaled_i.iloc[:,X3_vars]\n","                y3 = df_with_i.iloc[:,3]                # CHANGED FROM y1 = df_scaled.iloc[:,0] for categorical\n","                groups3 = df_scaled_i['fid']            # CHANGED FROM df_scaled.columns TO GET CORRECT # FAMID/GROUPS\n","                features_full2 = ['Age']\n","                for i in [X3_vars[1:]]:\n","                    features_full2.extend(df_scaled_i.columns[i])\n","\n","\n","            ##############################\n","            # CROSS-VALIDATION SELECTION #\n","            ##############################\n","\n","            inner_rstate = 9052017\n","            outer_rstate = 12301985\n","            #inner_rstate = np.random.RandomState(9052017)\n","            #outer_rstate = np.random.RandomState(12301985)\n","\n","            cv=GroupShuffleSplit\n","            #cv=GroupKFold\n","            cv_inner = cv(n_splits=20, test_size=0.2, random_state=inner_rstate)\n","            cv_outer = cv(n_splits=10, test_size=0.2, random_state=outer_rstate)\n","            n_iter=200\n","            n_iter_str = str(n_iter)\n","\n","            print(\"Features Selected\")\n","\n","\n","            #########################################\n","            ### RESULTS WITH DEMOGRAPHIC FEATURES ###\n","            #########################################\n","\n","            for train1, test1 in cv_inner.split(X1, y1, groups=groups1):\n","                X_train1, X_test1 = X1.values[train1], X1.values[test1]\n","                y_train1, y_test1 = y1[train1], y1[test1]\n","                groups_train1, groups_test1 = groups1[train1], groups1[test1]\n","\n","            search_time_start1 = time.time()\n","            if estimator == 5:\n","                clf1 = alg\n","            elif estimator == 4:\n","                spw1 = (len(y1)-np.sum(y1))/(np.sum(y1))\n","                alg1 = XGBClassifier(scale_pos_weight=spw1)\n","                clf1 = RandomizedSearchCV(alg1, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf1.fit(X_train1, y_train1, groups = groups_train1)\n","            else:\n","                clf1 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf1.fit(X_train1, y_train1, groups = groups_train1)\n","\n","            search_time_stop1 = time.time()\n","\n","            if estimator == 5:\n","                cvalscorer1 = clf1\n","            else:\n","                cvalscorer1 = clf1.best_estimator_\n","\n","            train1_scores = cross_val_score(cvalscorer1, X_train1, y_train1, groups=groups_train1, cv=cv_inner, n_jobs=4)\n","            test1_scores = cross_val_score(cvalscorer1, X_test1, y_test1, groups=groups_test1, cv=cv_outer, n_jobs=4)\n","\n","            #################################\n","            ### RESULTS WITH ALL FEATURES ###\n","            #################################\n","\n","            for train2, test2 in cv_inner.split(X2, y2, groups=groups2):\n","                X_train2, X_test2 = X2.values[train2], X2.values[test2]\n","                y_train2, y_test2 = y2[train2], y2[test2]\n","                groups_train2, groups_test2 = groups2[train2], groups2[test2]\n","\n","            search_time_start2 = time.time()\n","            if estimator == 5:\n","                clf2 = alg\n","            elif estimator == 4:\n","                spw2 = (len(y2)-np.sum(y2))/(np.sum(y2))\n","                alg2 = XGBClassifier(scale_pos_weight=spw2)\n","                clf2 = RandomizedSearchCV(alg2, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf2.fit(X_train2, y_train2, groups = groups_train2)\n","            else:\n","                clf2 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                clf2.fit(X_train2, y_train2, groups = groups_train2)\n","\n","            search_time_stop2 = time.time()\n","\n","            if estimator == 5:\n","                cvalscorer2 = clf2\n","            else:\n","                cvalscorer2 = clf2.best_estimator_\n","\n","            train2_scores = cross_val_score(cvalscorer2, X_train2, y_train2, groups=groups_train2, cv=cv_inner, n_jobs=4)\n","            test2_scores = cross_val_score(cvalscorer2, X_test2, y_test2, groups=groups_test2, cv=cv_outer, n_jobs=4)\n","\n","            if estimator != 5:\n","                print(\"Best Demo Estimator:\\n\", clf1.best_estimator_)\n","                print(\"\")\n","                print(\"Best Demo Model params:\")\n","                best_params1 = clf1.best_params_\n","                for param_name in sorted(best_params1.keys()):\n","                    print('%s: %r' % (param_name, best_params1[param_name]))\n","\n","            if estimator != 5:\n","                print(\"Best Full Estimator:\\n\", clf2.best_estimator_)\n","                print(\"\")\n","                print(\"Best Full params:\")\n","                best_params2 = clf2.best_params_\n","                for param_name in sorted(best_params2.keys()):\n","                    print('%s: %r' % (param_name, best_params2[param_name]))\n","\n","            ###########################################\n","            ### RESULTS WITH ALL FEATURES & IMAGING ###\n","            ###########################################\n","\n","            if data == 3 or data == 4:\n","\n","                for train3, test3 in cv_inner.split(X3, y3, groups=groups3):\n","                    X_train3, X_test3 = X3.values[train3], X3.values[test3]\n","                    y_train3, y_test3 = y3[train3], y3[test3]\n","                    groups_train3, groups_test3 = groups3[train3], groups3[test3]\n","\n","                search_time_start3 = time.time()\n","                if estimator == 5:\n","                    clf3 = alg\n","                elif estimator == 4:\n","                    spw3 = (len(y3)-np.sum(y3))/(np.sum(y3))\n","                    alg3 = XGBClassifier(scale_pos_weight=spw3)\n","                    clf3 = RandomizedSearchCV(alg3, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                    clf3.fit(X_train3, y_train3, groups = groups_train3)\n","                else:\n","                    clf3 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","                    clf3.fit(X_train3, y_train3, groups = groups_train3)\n","                search_time_stop3 = time.time()\n","\n","                if estimator == 5:\n","                    cvalscorer3 = clf3\n","                else:\n","                    cvalscorer3 = clf3.best_estimator_\n","\n","                train3_scores = cross_val_score(cvalscorer3, X_train3, y_train3, groups=groups_train3, cv=cv_inner, n_jobs=4)\n","                test3_scores = cross_val_score(cvalscorer3, X_test3, y_test3, groups=groups_test3, cv=cv_outer, n_jobs=4)\n","\n","                if estimator != 5:\n","                    print(\"Best Full Estimator:\\n\", clf3.best_estimator_)\n","                    print(\"\")\n","                    print(\"Best Full params:\")\n","                    best_params3 = clf3.best_params_\n","                    for param_name in sorted(best_params3.keys()):\n","                        print('%s: %r' % (param_name, best_params3[param_name]))\n","            else:\n","                print('No Imaging Data in CLDRC')\n","\n","\n","            #################################\n","            ###   FULL CONTENT TO PRINT   ###\n","            #################################\n","\n","            print(\"Dataset_Name: \", dataset_name)\n","            print(\"Model_Variable: \", model_var)\n","            print(\"Model_Estimator: \", model_estimator)\n","            print(\"Imaging_File: \", i_filename)\n","            print(\"\")\n","            print(\"Data_Preprocessed_-_Scaler_Applied\")\n","            print(\"Features_Selected\")\n","            print(\"Inner_Training_CV: \", cv_inner)\n","            print(\"Outer_Testing_CV: \", cv_outer)\n","            print(\"\")\n","\n","            print(\"Model_Tuning_Times: \\n\")\n","            print(\"Demographics_tuning_time: \")\n","            print(\"%.3f\" %(search_time_stop1 - search_time_start1))\n","            print(\"\")\n","            print(\"Full_Model_tuning_time: \")\n","            print(\"%.3f\" %(search_time_stop2 - search_time_start2))\n","            print(\"\")\n","\n","            if data == 3 or data == 4:\n","                print(\"Full_Model_&_Imaging_tuning_time: \")\n","                print(\"%.3f\" %(search_time_stop3 - search_time_start3))\n","            print(\"\")\n","\n","            print(\"DEMOGRAPHIC_MODEL\")\n","            print(\"\")\n","            if estimator == 5:\n","                print(\"No Best Estimator for LinearRegression\")\n","            else:\n","                print(\"Demo_Best_Estimator:\\n\", clf1.best_estimator_)\n","            print(\"\")\n","            print(\"Demo_Train_R2s_Mean:\\n%.3f\" %train1_scores.mean())\n","            print(\"Demo_Train_R2s_SD:\\n%.3f\" %train1_scores.std())\n","            print(\"\")\n","            print(\"Demo_Test_R2s_Mean:\\n%.3f\" %test1_scores.mean())\n","            print(\"Demo_Test_R2s_SD:\\n%.3f\" %test1_scores.std())\n","            print(\"\")\n","\n","            print(\"FULL_MODEL\")\n","            print(\"\")\n","            if estimator == 5:\n","                print(\"No Best Estimator for LinearRegression\")\n","            else:\n","                print(\"Full_Best_Estimator:\\n\", clf2.best_estimator_)\n","            print(\"\")\n","            print(\"Full_Train_R2s_Mean:\\n%.3f\" %train2_scores.mean())\n","            print(\"Full_Train_R2s_SD:\\n%.3f\" %train2_scores.std())\n","            print(\"\")\n","            print(\"Full_Test_R2s_Mean:\\n%.3f\" %test2_scores.mean())\n","            print(\"Full_Test_R2s_SD:\\n%.3f\" %test2_scores.std())\n","            print(\"\")\n","\n","            if data == 3 or data == 4:\n","                print(\"FULL_&_IMAGING_MODEL\")\n","                print(\"\")\n","                if estimator == 5:\n","                    print(\"No Best Estimator for LinearRegression\")\n","                else:\n","                    print(\"Imaging_Best_Estimator:\\n\", clf3.best_estimator_)\n","                print(\"\")\n","                print(\"Imaging_Train_R2s_Mean:\\n%.3f\" %train3_scores.mean())\n","                print(\"Imaging_Train_R2s_SD:\\n%.3f\" %train3_scores.std())\n","                print(\"\")\n","                print(\"Imaging_Test_R2s_Mean:\\n%.3f\" %test3_scores.mean())\n","                print(\"Imaging_Test_R2s_SD:\\n%.3f\" %test3_scores.std())\n","                print(\"\")\n","\n","\n","            ############################\n","            #### DIAGNOSTIC ACCURACY ###\n","            ############################\n","\n","            cvreg=StratifiedShuffleSplit\n","            cvreg_inner = cvreg(n_splits=20, test_size=0.333, random_state=123085)\n","            cvreg_outer = cvreg(n_splits=5, test_size=0.333, random_state=90517)\n","\n","            if estimator == 5:\n","                cvalscorer1 = clf1\n","                cvalscorer2 = clf2\n","                if data == 3 or data == 4:\n","                    cvalscorer3 = clf3\n","            else:\n","                cvalscorer1 = clf1.best_estimator_\n","                cvalscorer2 = clf2.best_estimator_\n","                if data == 3 or data == 4:\n","                    cvalscorer3 = clf3.best_estimator_     # CURRENT ERROR POINT #\n","\n","            cm1_score=[]\n","            cm1_truepos=[]\n","            cm1_falseneg=[]\n","            cm1_falsepos=[]\n","            cm1_trueneg=[]\n","            cm1f1_weight=[]\n","            sensitivity1_avg=[]\n","\n","            cm2_score=[]\n","            cm2_truepos=[]\n","            cm2_falseneg=[]\n","            cm2_falsepos=[]\n","            cm2_trueneg=[]\n","            cm2f1_weight=[]\n","            sensitivity2_avg=[]\n","\n","            if data == 3 or data == 4:\n","                cm3_score=[]\n","                cm3_truepos=[]\n","                cm3_falseneg=[]\n","                cm3_falsepos=[]\n","                cm3_trueneg=[]\n","                cm3f1_weight=[]\n","                sensitivity3_avg=[]\n","\n","            print(dataset_name, model_estimator, model_var, \"Diagnostic Accuracy\")\n","\n","            print('')\n","            print('DEMOGRAPHICS MODEL ACCURACY')\n","            for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X1, y1, groups=groups1)):\n","                cvalscorer1.fit(X1.values[trainCV], y1[trainCV])\n","                y1_pred = cvalscorer1.predict(X1.values[testCV])\n","                cm1 = confusion_matrix(y1[testCV],y1_pred, labels=[1,0])\n","                accuracy1=(cm1[0,0]+cm1[1,1])/sum(sum(cm1))\n","                sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","                specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","                if variable == 5:\n","                    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg0: {8:.4f}, f1_avg1: {9:.4f}, f1_avg2: {10:.4f}, f1_avg3: {11:.4f}, f1_weight: {12:.4f}\".\n","                          format(k, cvalscorer1.score(X1.values[testCV], y1[testCV]), sum(y1[testCV]), cm1[0,0], cm1[1,1], accuracy1, sensitivity1, specificity1, f1_score(y1[testCV],y1_pred,average=None)[0], f1_score(y1[testCV],y1_pred,average=None)[1], f1_score(y1[testCV],y1_pred,average=None)[2], f1_score(y1[testCV],y1_pred,average=None)[3], f1_score(y1[testCV],y1_pred,average='weighted')))\n","                else:\n","                    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg: {8:.4f}, f1_weight: {9:.4f}\".\n","                          format(k, cvalscorer1.score(X1.values[testCV], y1[testCV]), sum(y1[testCV]), cm1[0,0], cm1[1,1], accuracy1, sensitivity1, specificity1, f1_score(y1[testCV],y1_pred,average='binary'), f1_score(y1[testCV],y1_pred,average='weighted')))\n","                cm1_score.append(cvalscorer1.score(X1.values[testCV], y1[testCV]))\n","                cm1_truepos.append(cm1[0,0])\n","                cm1_falseneg.append(cm1[0,1])\n","                cm1_falsepos.append(cm1[1,0])\n","                cm1_trueneg.append(cm1[1,1])\n","                cm1f1_weight.append(f1_score(y1[testCV],y1_pred,average='weighted'))\n","                sensitivity1_avg.append(sensitivity1)\n","\n","            print('')\n","            cm1_avg = [[np.mean(cm1_truepos),np.mean(cm1_falseneg)],[np.mean(cm1_falsepos),np.mean(cm1_trueneg)]]\n","            print(cm1_avg[0])\n","            print(cm1_avg[1])\n","            print('Demo Average Accuracy: ',np.mean(cm1_score))\n","            print('Demo Weighted F1 M: ',np.mean(cm1f1_weight))\n","            print('Demo Weighted F1 SD: ',np.std(cm1f1_weight))\n","            print('Demo Sensitivity M: ',np.mean(sensitivity1_avg))\n","            print('Demo Sensitivity SD: ',np.std(sensitivity1_avg))\n","            #print('Demo Sensitivity: ',np.mean(sensitivity2_avg))\n","#            print('Demo Precision: ',cm1_avg[0,0]/(cm1_avg[0,0]+cm1_avg[1,0]))\n","#            print('Demo Recall: ',cm1_avg[0,0]/(cm1_avg[0,0]+cm1_avg[0,1]))\n","\n","            print('\\n'*1)\n","            print('FULL MODEL ACCURACY')\n","            for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","                cvalscorer2.fit(X2.values[trainCV], y2[trainCV])\n","                y2_pred = cvalscorer2.predict(X2.values[testCV])\n","                cm2 = confusion_matrix(y2[testCV],y2_pred, labels=[1,0])\n","                accuracy2=(cm2[0,0]+cm2[1,1])/sum(sum(cm2))\n","                sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[0,1])\n","                specificity2 = cm2[1,1]/(cm2[1,0]+cm2[1,1])\n","                if variable == 5:\n","                    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg0: {8:.4f}, f1_avg1: {9:.4f}, f1_avg2: {10:.4f}, f1_avg3: {11:.4f}, f1_weight: {12:.4f}\".\n","                          format(k, cvalscorer2.score(X2.values[testCV], y2[testCV]), sum(y2[testCV]), cm2[0,0], cm2[1,1], accuracy2, sensitivity2, specificity2, f1_score(y2[testCV],y2_pred,average=None)[0], f1_score(y2[testCV],y2_pred,average=None)[1], f1_score(y2[testCV],y2_pred,average=None)[2], f1_score(y2[testCV],y2_pred,average=None)[3], f1_score(y2[testCV],y2_pred,average='weighted')))\n","                else:\n","                    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg: {8:.4f}, f1_weight: {9:.4f}\".\n","                          format(k, cvalscorer2.score(X2.values[testCV], y2[testCV]), sum(y2[testCV]), cm2[0,0], cm2[1,1], accuracy2, sensitivity2, specificity2, f1_score(y2[testCV],y2_pred,average='binary'), f1_score(y2[testCV],y2_pred,average='weighted')))\n","                cm2_score.append(cvalscorer2.score(X2.values[testCV], y2[testCV]))\n","                cm2_truepos.append(cm2[0,0])\n","                cm2_falseneg.append(cm2[0,1])\n","                cm2_falsepos.append(cm2[1,0])\n","                cm2_trueneg.append(cm2[1,1])\n","                cm2f1_weight.append(f1_score(y2[testCV],y2_pred,average='weighted'))\n","                sensitivity2_avg.append(sensitivity2)\n","\n","            print('')\n","            cm2_avg = [[np.mean(cm2_truepos),np.mean(cm2_falseneg)],[np.mean(cm2_falsepos),np.mean(cm2_trueneg)]]\n","            print(cm2_avg[0])\n","            print(cm2_avg[1])\n","            print('Full Average Accuracy: ',np.mean(cm2_score))\n","            print('Full Weighted F1 M: ',np.mean(cm2f1_weight))\n","            print('Full Weighted F1 SD: ',np.std(cm2f1_weight))\n","            print('Full Sensitivity M: ',np.mean(sensitivity2_avg))\n","            print('Full Sensitivity SD: ',np.std(sensitivity2_avg))\n","#            print('Full Precision: ',cm2_avg[0,0]/(cm2_avg[0,0]+cm2_avg[1,0]))\n","#            print('Full Recall: ',cm2_avg[0,0]/(cm2_avg[0,0]+cm2_avg[0,1]))\n","\n","            if data == 3 or data == 4:\n","                print('\\n'*1)\n","                print('FULL + IMAGING MODEL ACCURACY')\n","                for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","                    cvalscorer3.fit(X3.values[trainCV], y3[trainCV])\n","                    y3_pred = cvalscorer3.predict(X3.values[testCV])\n","                    cm3 = confusion_matrix(y3[testCV],y3_pred, labels=[1,0])\n","                    accuracy3=(cm3[0,0]+cm3[1,1])/sum(sum(cm3))\n","                    sensitivity3 = cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","                    specificity3 = cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","                    if variable == 5:\n","                        print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg0: {8:.4f}, f1_avg1: {9:.4f}, f1_avg2: {10:.4f}, f1_avg3: {11:.4f}, f1_weight: {12:.4f}\".\n","                              format(k, cvalscorer3.score(X3.values[testCV], y3[testCV]), sum(y3[testCV]), cm3[0,0], cm3[1,1], accuracy3, sensitivity3, specificity3, f1_score(y3[testCV],y3_pred,average=None)[0], f1_score(y3[testCV],y3_pred,average=None)[1], f1_score(y3[testCV],y3_pred,average=None)[2], f1_score(y3[testCV],y3_pred,average=None)[3], f1_score(y3[testCV],y3_pred,average='weighted')))\n","                    else:\n","                        print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg: {8:.4f}, f1_weight: {9:.4f}\".\n","                              format(k, cvalscorer3.score(X3.values[testCV], y3[testCV]), sum(y3[testCV]), cm3[0,0], cm3[1,1], accuracy3, sensitivity3, specificity3, f1_score(y3[testCV],y3_pred,average='binary'), f1_score(y3[testCV],y3_pred,average='weighted')))\n","                    cm3_score.append(cvalscorer3.score(X3.values[testCV], y3[testCV]))\n","                    cm3_truepos.append(cm3[0,0])\n","                    cm3_falseneg.append(cm3[0,1])\n","                    cm3_falsepos.append(cm3[1,0])\n","                    cm3_trueneg.append(cm3[1,1])\n","                    cm3f1_weight.append(f1_score(y3[testCV],y3_pred,average='weighted'))\n","                    sensitivity3_avg.append(sensitivity3)\n","\n","                print('')\n","                cm3_avg = [[np.mean(cm3_truepos),np.mean(cm3_falseneg)],[np.mean(cm3_falsepos),np.mean(cm3_trueneg)]]\n","                print(cm3_avg[0])\n","                print(cm3_avg[1])\n","                print('Imaging Average Accuracy: ',np.mean(cm3_score))\n","                print('Imaging Weighted F1 M: ',np.mean(cm3f1_weight))\n","                print('Imaging Weighted F1 SD: ',np.std(cm3f1_weight))\n","                print('Imaging Sensitivity M: ',np.mean(sensitivity3_avg))\n","                print('Imaging Sensitivity SD: ',np.std(sensitivity3_avg))\n","#                print('Imaging Precision: ',cm3_avg[0,0]/(cm3_avg[0,0]+cm3_avg[1,0]))\n","#                print('Imaging Recall: ',cm3_avg[0,0]/(cm3_avg[0,0]+cm3_avg[0,1]))\n","            else:\n","                print('Imaging not in CLDRC')\n","            print('')\n","            print('')\n","            print('')\n","            print('')\n","            print('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHdVhB_K97hu"},"outputs":[],"source":["if variable == 1:\n","    with open(\"0.results_dxaccuracy_estimators_READ_f1recall_noHubDropNA.txt\", 'w') as f:\n","        f.write(capture.stdout)\n","elif variable == 2:\n","    with open(\"0.results_dxaccuracy_estimators_IN_f1recall_noHubDropNA.txt\", 'w') as f:\n","        f.write(capture.stdout)\n","elif variable == 3:\n","    with open(\"0.results_dxaccuracy_estimators_HI_f1recall_noHubDropNA.txt\", 'w') as f:\n","        f.write(capture.stdout)\n","elif variable == 4:\n","    with open(\"0.results_dxaccuracy_estimators_ADHD_f1recall_noHubDropNA.txt\", 'w') as f:\n","        f.write(capture.stdout)\n","elif variable == 5:\n","    with open(\"0.results_dxaccuracy_estimators_ADHDTYPE_f1recall_noHubDropNA.txt\", 'w') as f:\n","        f.write(capture.stdout)\n","else:\n","    print('Variable not correctly set')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OlbF7xNq97hv","executionInfo":{"status":"ok","timestamp":1687804294408,"user_tz":360,"elapsed":4,"user":{"displayName":"Daniel Leopold","userId":"11432218311588552018"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FcbJkNhN97hv"},"outputs":[],"source":["# ORIGINAL DIAGNOSTIC ACCURACY CODE\n","\n","############################\n","#### DIAGNOSTIC ACCURACY ###\n","############################\n","\n","cvreg=StratifiedShuffleSplit\n","cvreg_inner = cvreg(n_splits=20, test_size=0.333, random_state=123085)\n","cvreg_outer = cvreg(n_splits=5, test_size=0.333, random_state=90517)\n","\n","if estimator == 5:\n","    cvalscorer1 = clf1\n","    cvalscorer2 = clf2\n","    cvalscorer3 = clf3\n","else:\n","    cvalscorer1 = clf1.best_estimator_\n","    cvalscorer2 = clf2.best_estimator_\n","    cvalscorer3 = clf3.best_estimator_\n","\n","cm1_score=[]\n","cm1_truepos=[]\n","cm1_falseneg=[]\n","cm1_falsepos=[]\n","cm1_trueneg=[]\n","cm1f1_weight=[]\n","\n","cm2_score=[]\n","cm2_truepos=[]\n","cm2_falseneg=[]\n","cm2_falsepos=[]\n","cm2_trueneg=[]\n","cm2f1_weight=[]\n","\n","cm3_score=[]\n","cm3_truepos=[]\n","cm3_falseneg=[]\n","cm3_falsepos=[]\n","cm3_trueneg=[]\n","cm3f1_weight=[]\n","\n","print(dataset_name, model_estimator, model_var, \"Diagostic Accuracy\")\n","\n","print('')\n","print('DEMOGRAPHICS MODEL ACCURACY')\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X1, y1, groups=groups1)):\n","    cvalscorer1.fit(X1.values[trainCV], y1[trainCV])\n","    y1_pred = cvalscorer1.predict(X1.values[testCV])\n","    cm1 = confusion_matrix(y1[testCV],y1_pred, labels=[1,0])\n","    accuracy1=(cm1[0,0]+cm1[1,1])/sum(sum(cm1))\n","    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg {8:.4f}, f1_weight {9:.4f}\".\n","          format(k, cvalscorer1.score(X1.values[testCV], y1[testCV]), sum(y1[testCV]), cm1[0,0], cm1[1,1], accuracy1, sensitivity1, specificity1, f1_score(y1[testCV],y1_pred,average='binary'), f1_score(y1[testCV],y1_pred,average='weighted')))\n","    cm1_score.append(cvalscorer1.score(X1.values[testCV], y1[testCV]))\n","    cm1_truepos.append(cm1[0,0])\n","    cm1_falseneg.append(cm1[0,1])\n","    cm1_falsepos.append(cm1[1,0])\n","    cm1_trueneg.append(cm1[1,1])\n","    cm1f1_weight.append(f1_score(y1[testCV],y1_pred,average='weighted'))\n","\n","print('')\n","cm1_avg = [[np.mean(cm1_truepos),np.mean(cm1_falseneg)],[np.mean(cm1_falsepos),np.mean(cm1_trueneg)]]\n","print(cm1_avg[0])\n","print(cm1_avg[1])\n","print('Demo Average Accuracy: ',np.mean(cm1_score))\n","print('Demo Weighted F1: ',np.mean(cm1f1_weight))\n","\n","print('\\n'*1)\n","print('FULL MODEL ACCURACY')\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","    cvalscorer2.fit(X2.values[trainCV], y2[trainCV])\n","    y2_pred = cvalscorer2.predict(X2.values[testCV])\n","    cm2 = confusion_matrix(y2[testCV],y2_pred, labels=[1,0])\n","    accuracy2=(cm2[0,0]+cm2[1,1])/sum(sum(cm2))\n","    sensitivity2 = cm2[0,0]/(cm2[0,0]+cm2[0,1])\n","    specificity2 = cm2[1,1]/(cm2[1,0]+cm2[1,1])\n","    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg {8:.4f}, f1_weight {9:.4f}\".\n","          format(k, cvalscorer2.score(X2.values[testCV], y2[testCV]), sum(y2[testCV]), cm2[0,0], cm2[1,1], accuracy2, sensitivity2, specificity2, f1_score(y2[testCV],y2_pred,average='binary'), f1_score(y2[testCV],y2_pred,average='weighted')))\n","    cm2_score.append(cvalscorer2.score(X2.values[testCV], y2[testCV]))\n","    cm2_truepos.append(cm2[0,0])\n","    cm2_falseneg.append(cm2[0,1])\n","    cm2_falsepos.append(cm2[1,0])\n","    cm2_trueneg.append(cm2[1,1])\n","    cm2f1_weight.append(f1_score(y2[testCV],y2_pred,average='weighted'))\n","\n","print('')\n","cm2_avg = [[np.mean(cm2_truepos),np.mean(cm2_falseneg)],[np.mean(cm2_falsepos),np.mean(cm2_trueneg)]]\n","print(cm2_avg[0])\n","print(cm2_avg[1])\n","print('Full Average Accuracy: ',np.mean(cm2_score))\n","print('Full Weighted F1: ',np.mean(cm2f1_weight))\n","\n","print('\\n'*1)\n","print('FULL + IMAGING MODEL ACCURACY')\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","    cvalscorer3.fit(X3.values[trainCV], y3[trainCV])\n","    y3_pred = cvalscorer3.predict(X3.values[testCV])\n","    cm3 = confusion_matrix(y3[testCV],y3_pred, labels=[1,0])\n","    accuracy3=(cm3[0,0]+cm3[1,1])/sum(sum(cm3))\n","    sensitivity3 = cm3[0,0]/(cm3[0,0]+cm3[0,1])\n","    specificity3 = cm3[1,1]/(cm3[1,0]+cm3[1,1])\n","    print(\"[fold {0}] score: {1:.4f}, N_dx: {2}, truepos: {3}, trueneg: {4}, acc: {5:.4f}, sens: {6:.4f}, spec: {7:.4f}, f1_avg {8:.4f}, f1_weight {9:.4f}\".\n","          format(k, cvalscorer3.score(X3.values[testCV], y3[testCV]), sum(y3[testCV]), cm3[0,0], cm3[1,1], accuracy3, sensitivity3, specificity3, f1_score(y3[testCV],y3_pred,average='binary'), f1_score(y3[testCV],y3_pred,average='weighted')))\n","    cm3_score.append(cvalscorer3.score(X3.values[testCV], y3[testCV]))\n","    cm3_truepos.append(cm3[0,0])\n","    cm3_falseneg.append(cm3[0,1])\n","    cm3_falsepos.append(cm3[1,0])\n","    cm3_trueneg.append(cm3[1,1])\n","    cm3f1_weight.append(f1_score(y3[testCV],y3_pred,average='weighted'))\n","\n","print('')\n","cm3_avg = [[np.mean(cm3_truepos),np.mean(cm3_falseneg)],[np.mean(cm3_falsepos),np.mean(cm3_trueneg)]]\n","print(cm3_avg[0])\n","print(cm3_avg[1])\n","print('Imaging Average Accuracy: ',np.mean(cm3_score))\n","print('Imaging Weighted F1: ',np.mean(cm3f1_weight))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o7f2L6pY97hv"},"outputs":[],"source":["with open(\"0.results_dxaccuracy_estimators_READ.txt\", 'w') as f:\n","    f.write(capture.stdout)\n","\n","#with open(\"0.results_dxaccuracy_estimators_IN.txt\", 'w') as f:\n","#    f.write(capture.stdout)\n","\n","#with open(\"0.results_dxaccuracy_estimators_HI.txt\", 'w') as f:\n","#    f.write(capture.stdout)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d5XtTHXd97hw"},"outputs":[],"source":["#########################################\n","### RESULTS WITH DEMOGRAPHIC FEATURES ###\n","#########################################\n","\n","print(\"Dataset: \", dataset_name)\n","print(\"Model Variable: \", model_var)\n","print(\"Model Estimator: \", model_estimator)\n","print(\"\")\n","\n","#print(\"Model Tuning Times: \\n\")\n","\n","#for train1, test1 in cv_inner.split(X1, y1, groups=groups1):\n","#    X_train1, X_test1 = X1.values[train1], X1.values[test1]\n","#    y_train1, y_test1 = y1[train1], y1[test1]\n","#    groups_train1, groups_test1 = groups1[train1], groups1[test1]\n","\n","#search_time_start1 = time.time()\n","#print(\"Demographics tuning time...\")\n","#if estimator == 5:\n","#    clf1 = alg\n","#else:\n","#    clf1 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","#    clf1.fit(X_train1, y_train1, groups = groups_train1)\n","\n","#search_time_stop1 = time.time()\n","#print('%.3f' %(search_time_stop1 - search_time_start1))\n","#print(\"\")\n","\n","#if estimator == 5:\n","#    cvalscorer1 = clf1\n","#else:\n","#    cvalscorer1 = clf1.best_estimator_\n","\n","#train1_scores = cross_val_score(cvalscorer1, X_train1, y_train1, groups=groups_train1, cv=cv_inner, n_jobs=4)\n","#test1_scores = cross_val_score(cvalscorer1, X_test1, y_test1, groups=groups_test1, cv=cv_outer, n_jobs=4)\n","\n","#################################\n","### RESULTS WITH ALL FEATURES ###\n","#################################\n","\n","for train2, test2 in cv_inner.split(X2, y2, groups=groups2):\n","    X_train2, X_test2 = X2.values[train2], X2.values[test2]\n","    y_train2, y_test2 = y2[train2], y2[test2]\n","    groups_train2, groups_test2 = groups2[train2], groups2[test2]\n","    r_train2, r_test2 = r2[train2], r2[test2]\n","    i_train2, i_test2 = i2[train2], i2[test2]\n","    h_train2, h_test2 = h2[train2], h2[test2]\n","    m_train2, m_test2 = m2[train2], m2[test2]\n","\n","#search_time_start2 = time.time()\n","#print(\"Full Model tuning time...\")\n","#if estimator == 5:\n","#    clf2 = alg\n","#else:\n","#    clf2 = RandomizedSearchCV(alg, param_dist, n_iter=n_iter, iid=False, cv=cv_inner, n_jobs=4)\n","#    clf2.fit(X_train2, y_train2, groups = groups_train2)\n","\n","#search_time_stop2 = time.time()\n","#print('%.3f' %(search_time_stop2 - search_time_start2))\n","#print(\"\\n\"*2)\n","\n","#if estimator == 5:\n","#    cvalscorer2 = clf2\n","#else:\n","#    cvalscorer2 = clf2.best_estimator_\n","\n","train2_scores = cross_val_score(lasso_cvb, X_train2, y_train2, groups=groups_train2, cv=cv_inner, n_jobs=4)\n","test2_scores = cross_val_score(lasso_cvb, X_test2, y_test2, groups=groups_test2, cv=cv_outer, n_jobs=4)\n","\n","#print(\"DEMOGRAPHIC MODEL RESULTS\")\n","#print(\"\")\n","#print(\"Train [inner CV] R2s:\\n\", train1_scores)\n","#print(\"Train R2s Mean:\\n%.3f\" %train1_scores.mean())\n","#print(\"Train R2s SD:\\n%.3f\" %train1_scores.std())\n","#print(\"\")\n","#print(\"Test [outer CV] R2s:\\n\", test1_scores)\n","#print(\"Test R2s Mean:\\n%.3f\" %test1_scores.mean())\n","#print(\"Test R2s SD:\\n%.3f\" %test1_scores.std())\n","#print(\"\")\n","#if estimator != 5:\n","#    print(\"Best Demo Estimator:\\n\", clf1.best_estimator_)\n","#    print(\"\")\n","#    print(\"Best Demo Model params:\")\n","#    best_params1 = clf1.best_params_\n","#    for param_name in sorted(best_params1.keys()):\n","#        print('%s: %r' % (param_name, best_params1[param_name]))\n","print(\"\\n\"*2)\n","print(\"FULL BEHAVIORAL MODEL RESULTS\")\n","print(\"\")\n","print(\"Train [inner CV] R2s:\\n\", train2_scores)\n","print(\"Train R2s Mean:\\n%.3f\" %train2_scores.mean())\n","print(\"Train R2s SD:\\n%.3f\" %train2_scores.std())\n","print(\"\")\n","print(\"Test [outer CV] R2s:\\n\", test2_scores)\n","print(\"Test R2s Mean:\\n%.3f\" %test2_scores.mean())\n","print(\"Test R2s SD:\\n%.3f\" %test2_scores.std())\n","print(\"\")\n","#if estimator != 5:\n","#    print(\"Best Full Estimator:\\n\", clf2.best_estimator_)\n","#    print(\"\")\n","#    print(\"Best Full params:\")\n","#    best_params2 = clf2.best_params_\n","#    for param_name in sorted(best_params2.keys()):\n","#        print('%s: %r' % (param_name, best_params2[param_name]))"]},{"cell_type":"markdown","metadata":{"id":"8kZENH6Z97iC"},"source":["# Feature Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C47Kp9Td97iC"},"outputs":[],"source":["print(\"MODEL FEATURE WEIGHTS\")\n","\n","print(\"Dataset: \", dataset_name)\n","print(\"Model Variable: \", model_var)\n","print(\"Model Estimator: \", model_estimator)\n","print(\"\\n\"*2)\n","\n","if estimator == 1:\n","    alg1_coef = LinearSVR(max_iter = 10000, C = clf1.best_params_['C'], epsilon = clf1.best_params_['epsilon'])\n","    alg1_for_coef = alg1_coef.fit(X_train1, y_train1)\n","    alg2_coef = LinearSVR(max_iter = 10000, C = clf2.best_params_['C'], epsilon = clf2.best_params_['epsilon'])\n","    alg2_for_coef = alg2_coef.fit(X_train2, y_train2)\n","    if data == 3 or data == 4:\n","        alg3_coef = LinearSVR(max_iter = 10000, C = clf3.best_params_['C'], epsilon = clf3.best_params_['epsilon'])\n","        alg3_for_coef = alg3_coef.fit(X_train3, y_train3)\n","elif estimator == 2:\n","    alg1_coef = SVR(kernel = 'linear', C = clf1.best_params_['C'], epsilon = clf1.best_params_['epsilon'])\n","    alg1_for_coef = alg1_coef.fit(X_train1, y_train1)\n","    alg2_coef = SVR(kernel = 'linear', C = clf2.best_params_['C'], epsilon = clf2.best_params_['epsilon'])\n","    alg2_for_coef = alg2_coef.fit(X_train2, y_train2)\n","    if data == 3 or data == 4:\n","        alg3_coef = SVR(kernel = 'linear', C = clf3.best_params_['C'], epsilon = clf3.best_params_['epsilon'])\n","        alg3_for_coef = alg3_coef.fit(X_train3, y_train3)\n","elif estimator == 3:\n","    alg1_coef = SVR(kernel = 'rbf', gamma = 'auto', C = clf1.best_params_['C'], epsilon = clf1.best_params_['epsilon'])\n","    alg1_for_coef = alg1_coef.fit(X_train1, y_train1)\n","    alg2_coef = SVR(kernel = 'rbf', gamma = 'auto', C = clf2.best_params_['C'], epsilon = clf2.best_params_['epsilon'])\n","    alg2_for_coef = alg2_coef.fit(X_train2, y_train2)\n","    if data == 3 or data == 4:\n","        alg3_coef = SVR(kernel = 'rbf', gamma = 'auto', C = clf3.best_params_['C'], epsilon = clf3.best_params_['epsilon'])\n","        alg3_for_coef = alg3_coef.fit(X_train3, y_train3)\n","elif estimator == 4:\n","    alg1_coef = XGBRegressor(learning_rate = clf1.best_params_['learning_rate'], gamma = clf1.best_params_['gamma'])\n","    alg1_coef = XGBRegressor(max_depth = clf1.best_params_['max_depth'], min_child_weight = clf1.best_params_['min_child_weight'])\n","    alg1_coef = XGBRegressor(subsample = clf1.best_params_['subsample'], colsample_bytree = clf1.best_params_['colsample_bytree'])\n","    alg1_coef = XGBRegressor(reg_lambda = clf1.best_params_['reg_lambda'], reg_alpha = clf1.best_params_['reg_alpha'])\n","    alg1_for_coef = alg1_coef.fit(X_train1, y_train1)\n","    alg2_coef = XGBRegressor(learning_rate = clf2.best_params_['learning_rate'], gamma = clf2.best_params_['gamma'])\n","    alg2_coef = XGBRegressor(max_depth = clf2.best_params_['max_depth'], min_child_weight = clf2.best_params_['min_child_weight'])\n","    alg2_coef = XGBRegressor(subsample = clf2.best_params_['subsample'], colsample_bytree = clf2.best_params_['colsample_bytree'])\n","    alg2_coef = XGBRegressor(reg_lambda = clf2.best_params_['reg_lambda'], reg_alpha = clf2.best_params_['reg_alpha'])\n","    alg2_for_coef = alg2_coef.fit(X_train2, y_train2)\n","    if data == 3 or data == 4:\n","        alg3_coef = XGBRegressor(learning_rate = clf3.best_params_['learning_rate'], gamma = clf3.best_params_['gamma'])\n","        alg3_coef = XGBRegressor(max_depth = clf3.best_params_['max_depth'], min_child_weight = clf3.best_params_['min_child_weight'])\n","        alg3_coef = XGBRegressor(subsample = clf3.best_params_['subsample'], colsample_bytree = clf3.best_params_['colsample_bytree'])\n","        alg3_coef = XGBRegressor(reg_lambda = clf3.best_params_['reg_lambda'], reg_alpha = clf3.best_params_['reg_alpha'])\n","        alg3_for_coef = alg3_coef.fit(X_train3, y_train3)\n","elif estimator == 5:\n","    alg1_for_coef = clf1.fit(X_train1, y_train1)\n","    alg2_for_coef = clf2.fit(X_train2, y_train2)\n","    if data == 3 or data == 4:\n","        alg3_for_coef = clf3.fit(X_train3, y_train3)\n","else:\n","    print(\"Estimator Not Selected\")\n","\n","feature_list_demo = []\n","feature_list_full = []\n","feature_list_imaging = []\n","\n","for i in list(range(0,len(X1.columns))):\n","    feature_list_demo.append([alg1_for_coef.coef_[i], X1.columns[i]])\n","    df_features_demo = pd.DataFrame(data=feature_list_demo, columns=['coef','feature'])\n","df_features_demo.reindex(df_features_demo.coef.abs().sort_values(ascending=False).index)\n","df_features_demo_sum = df_features_demo[(df_features_demo['coef'] > .001) | (df_features_demo['coef'] < -.001)]\n","df_features_demo_sum.reindex(df_features_demo_sum.coef.abs().sort_values(ascending=False).index)\n","\n","for i in list(range(0,len(X2.columns))):\n","    feature_list_full.append([alg2_for_coef.coef_[i], X2.columns[i]])\n","    df_features_full = pd.DataFrame(data=feature_list_full, columns=['coef','feature'])\n","df_features_full.reindex(df_features_full.coef.abs().sort_values(ascending=False).index)\n","df_features_full_sum = df_features_full[(df_features_full['coef'] > .001) | (df_features_full['coef'] < -.001)]\n","df_features_full_sum.reindex(df_features_full_sum.coef.abs().sort_values(ascending=False).index)\n","\n","for i in list(range(0,len(X3.columns))):\n","    feature_list_imaging.append([alg3_for_coef.coef_[i], X3.columns[i]])\n","    df_features_imaging = pd.DataFrame(data=feature_list_imaging, columns=['coef','feature'])\n","df_features_imaging.reindex(df_features_imaging.coef.abs().sort_values(ascending=False).index)\n","df_features_imaging_sum = df_features_imaging[(df_features_imaging['coef'] > .001) | (df_features_imaging['coef'] < -.001)]\n","df_features_imaging_sum.reindex(df_features_imaging_sum.coef.abs().sort_values(ascending=False).index)\n","\n","if estimator == 1 or estimator == 2:\n","    print(\"Demographic Feature Weights:\\n\", features_demo, \"\\n\", alg1_for_coef.coef_)\n","    print(\"\")\n","    print(\"All Feature Weights:\\n\", features_full1, \"\\n\", alg2_for_coef.coef_)\n","else:\n","    print(\"Weights only available for Linear Kernels\")\n","if (data == 3 or data == 4) and (estimator == 1 or estimator ==2):\n","    print(\"\")\n","    print(\"Full & Imaging Feature Weights:\\n\", features_full2, \"\\n\", alg3_for_coef.coef_)\n","else:\n","    print(\"Full model with imaging not available\")"]},{"cell_type":"markdown","metadata":{"id":"FB_1ieJd97iC"},"source":["# FULL REGULARIZATION STREAM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBFZPWpG97iD"},"outputs":[],"source":["from sklearn.model_selection import ShuffleSplit\n","\n","cvreg=ShuffleSplit\n","cvreg_inner = cvreg(n_splits=20, test_size=0.2, random_state=123085)\n","cvreg_outer = cvreg(n_splits=9, test_size=0.2, random_state=90517)\n","\n","##########################\n","### RidgeCV Behavioral ###\n","##########################\n","\n","from sklearn.linear_model import RidgeCV\n","\n","alphas_ridgecvb = np.logspace(-3, 2.3, 50)\n","ridge_cvb = RidgeCV(alphas=alphas_ridgecvb, cv=cvreg_inner, normalize=False)\n","ridge_cvb_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","    ridge_cvb.fit(X2.values[trainCV], y2[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, ridge_cvb.alpha_, ridge_cvb.score(X2.values[testCV], y2[testCV])))\n","    ridge_cvb_score.append(ridge_cvb.score(X2.values[testCV], y2[testCV]))\n","\n","print('RidgeCVB')\n","print('Mean: ', np.mean(ridge_cvb_score))\n","print('SD: ', np.std(ridge_cvb_score))\n","\n","feature_list_ridgeb = []\n","for i in list(range(0,len(X2.columns))):\n","    feature_list_ridgeb.append([ridge_cvb.coef_[i], X2.columns[i]])\n","    df_featimp_ridgeb = pd.DataFrame(data=feature_list_ridgeb, columns=['coef','feature'])\n","df_featimp_ridgeb.reindex(df_featimp_ridgeb.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_ridgeb = df_featimp_ridgeb[(df_featimp_ridgeb['coef'] > .001) | (df_featimp_ridgeb['coef'] < -.001)]\n","print(df_featimp_sum_ridgeb.reindex(df_featimp_sum_ridgeb.coef.abs().sort_values(ascending=False).index))\n","\n","#######################\n","### RidgeCV Imaging ###\n","#######################\n","\n","alphas_ridgecvi = np.logspace(-3, 3, 50)\n","ridge_cvi = RidgeCV(alphas=alphas_ridgecvi, cv=cvreg_inner, normalize=False)\n","ridge_cvi_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","    ridge_cvi.fit(X3.values[trainCV], y3[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, ridge_cvi.alpha_, ridge_cvi.score(X3.values[testCV], y3[testCV])))\n","    ridge_cvi_score.append(ridge_cvi.score(X3.values[testCV], y3[testCV]))\n","\n","print('RidgeCVI')\n","print('Mean: ', np.mean(ridge_cvi_score))\n","print('SD: ', np.std(ridge_cvi_score))\n","\n","feature_list_ridgei = []\n","for i in list(range(0,len(X3.columns))):\n","    feature_list_ridgei.append([ridge_cvi.coef_[i], X3.columns[i]])\n","    df_featimp_ridgei = pd.DataFrame(data=feature_list_ridgei, columns=['coef','feature'])\n","df_featimp_ridgei.reindex(df_featimp_ridgei.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_ridgei = df_featimp_ridgei[(df_featimp_ridgei['coef'] > .001) | (df_featimp_ridgei['coef'] < -.001)]\n","print(df_featimp_sum_ridgei.reindex(df_featimp_sum_ridgei.coef.abs().sort_values(ascending=False).index))\n","\n","##########################\n","### LassoCV Behavioral ###\n","##########################\n","\n","from sklearn.linear_model import LassoCV\n","\n","alphas_lassocvb = np.logspace(-4, -0.3, 100)\n","lasso_cvb = LassoCV(alphas=alphas_lassocvb, cv=cvreg_inner, max_iter=100000, tol=0.0001, n_jobs=4)\n","lasso_cvb_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","    lasso_cvb.fit(X2.values[trainCV], y2[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, lasso_cvb.alpha_, lasso_cvb.score(X2.values[testCV], y2[testCV])))\n","    lasso_cvb_score.append(lasso_cvb.score(X2.values[testCV], y2[testCV]))\n","\n","print('LassoCVB')\n","print('Mean: ', np.mean(lasso_cvb_score))\n","print('SD: ', np.std(lasso_cvb_score))\n","\n","feature_list_lassob = []\n","for i in list(range(0,len(X2.columns))):\n","    feature_list_lassob.append([lasso_cvb.coef_[i], X2.columns[i]])\n","    df_featimp_lassob = pd.DataFrame(data=feature_list_lassob, columns=['coef','feature'])\n","df_featimp_lassob.reindex(df_featimp_lassob.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_lassob = df_featimp_lassob[(df_featimp_lassob['coef'] > .001) | (df_featimp_lassob['coef'] < -.001)]\n","print(df_featimp_sum_lassob.reindex(df_featimp_sum_lassob.coef.abs().sort_values(ascending=False).index))\n","\n","#######################\n","### LassoCV Imaging ###\n","#######################\n","\n","alphas_lassocvi = np.logspace(-4, -0.3, 100)\n","lasso_cvi = LassoCV(alphas=alphas_lassocvi, cv=cvreg_inner, max_iter=100000, tol=0.0001, n_jobs=4)\n","lasso_cvi_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","    lasso_cvi.fit(X3.values[trainCV], y3[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, lasso_cvi.alpha_, lasso_cvi.score(X3.values[testCV], y3[testCV])))\n","    lasso_cvi_score.append(lasso_cvi.score(X3.values[testCV], y3[testCV]))\n","\n","print('LassoCVI')\n","print('Mean: ', np.mean(lasso_cvi_score))\n","print('SD: ', np.std(lasso_cvi_score))\n","\n","feature_list_lassoi = []\n","for i in list(range(0,len(X3.columns))):\n","    feature_list_lassoi.append([lasso_cvi.coef_[i], X3.columns[i]])\n","    df_featimp_lassoi = pd.DataFrame(data=feature_list_lassoi, columns=['coef','feature'])\n","df_featimp_lassoi.reindex(df_featimp_lassoi.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_lassoi = df_featimp_lassoi[(df_featimp_lassoi['coef'] > .001) | (df_featimp_lassoi['coef'] < -.001)]\n","print(df_featimp_sum_lassoi.reindex(df_featimp_sum_lassoi.coef.abs().sort_values(ascending=False).index))\n","\n","\n","##############################\n","### LassoLarsCV Behavioral ###\n","##############################\n","\n","from sklearn.linear_model import LassoLarsCV\n","\n","llars_cvb = LassoLarsCV(cv=cvreg_inner, max_iter=10000, n_jobs=4)\n","llars_cvb_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","    llars_cvb.fit(X2.values[trainCV], y2[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, llars_cvb.alpha_, llars_cvb.score(X2.values[testCV], y2[testCV])))\n","    llars_cvb_score.append(llars_cvb.score(X2.values[testCV], y2[testCV]))\n","\n","print('LassoLars CVB')\n","print('Mean: ', np.mean(llars_cvb_score))\n","print('SD: ', np.std(llars_cvb_score))\n","\n","feature_list_llb = []\n","for i in list(range(0,len(X2.columns))):\n","    feature_list_llb.append([llars_cvb.coef_[i], X2.columns[i]])\n","    df_featimp_llb = pd.DataFrame(data=feature_list_llb, columns=['coef','feature'])\n","df_featimp_llb.reindex(df_featimp_llb.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_llb = df_featimp_llb[(df_featimp_llb['coef'] > .001) | (df_featimp_llb['coef'] < -.001)]\n","print(df_featimp_sum_llb.reindex(df_featimp_sum_llb.coef.abs().sort_values(ascending=False).index))\n","\n","###########################\n","### LassoLarsCV Imaging ###\n","###########################\n","\n","llars_cvi = LassoLarsCV(cv=cvreg_inner, max_iter=10000, n_jobs=4)\n","llars_cvi_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","    llars_cvi.fit(X3.values[trainCV], y3[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, score: {2:.5f}\".\n","          format(k, llars_cvi.alpha_, llars_cvi.score(X3.values[testCV], y3[testCV])))\n","    llars_cvi_score.append(llars_cvi.score(X3.values[testCV], y3[testCV]))\n","\n","print('LassoLarsCVI')\n","print('Mean: ', np.mean(llars_cvi_score))\n","print('SD: ', np.std(llars_cvi_score))\n","\n","feature_list_lli = []\n","for i in list(range(0,len(X3.columns))):\n","    feature_list_lli.append([llars_cvi.coef_[i], X3.columns[i]])\n","    df_featimp_lli = pd.DataFrame(data=feature_list_lli, columns=['coef','feature'])\n","df_featimp_lli.reindex(df_featimp_lli.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_lli = df_featimp_lli[(df_featimp_lli['coef'] > .001) | (df_featimp_lli['coef'] < -.001)]\n","print(df_featimp_sum_lli.reindex(df_featimp_sum_lli.coef.abs().sort_values(ascending=False).index))\n","\n","###############################\n","### ElasticNetCV Behavioral ###\n","###############################\n","\n","from sklearn.linear_model import ElasticNetCV\n","\n","alphas_enetcvb = np.logspace(-3, -0.000001, 30)\n","l1s_enetcvb = np.logspace(-3,-0.000001,30)\n","enet_cvb = ElasticNetCV(alphas=alphas_enetcvb, l1_ratio=l1s_enetcvb, cv=cvreg_inner, max_iter=10000, normalize=False, n_jobs=4)\n","enet_cvb_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X2, y2, groups=groups2)):\n","    enet_cvb.fit(X2.values[trainCV], y2[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, l1_ratio: {2:.5f}, score: {3:.5f}\".\n","          format(k, enet_cvb.alpha_, enet_cvb.l1_ratio_, enet_cvb.score(X2.values[testCV], y2[testCV])))\n","    enet_cvb_score.append(enet_cvb.score(X2.values[testCV], y2[testCV]))\n","\n","print('ElasticNetCVB')\n","print('Mean: ', np.mean(enet_cvb_score))\n","print('SD: ', np.std(enet_cvb_score))\n","\n","feature_list_enb = []\n","for i in list(range(0,len(X2.columns))):\n","    feature_list_enb.append([enet_cvb.coef_[i], X2.columns[i]])\n","    df_featimp_enb = pd.DataFrame(data=feature_list_enb, columns=['coef','feature'])\n","df_featimp_enb.reindex(df_featimp_enb.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_enb = df_featimp_enb[(df_featimp_enb['coef'] > .001) | (df_featimp_enb['coef'] < -.001)]\n","print(df_featimp_sum_enb.reindex(df_featimp_sum_enb.coef.abs().sort_values(ascending=False).index))\n","\n","############################\n","### ElasticNetCV Imaging ###\n","############################\n","\n","cvreg_inner = cvreg(n_splits=30, test_size=0.2, random_state=12301985)\n","cvreg_outer = cvreg(n_splits=15, test_size=0.2, random_state=905017)\n","\n","alphas_enetcvi = np.logspace(-1.5, -.00001, 30)\n","l1s_enetcvi = np.logspace(-1,-.00001,30)\n","enet_cvi = ElasticNetCV(alphas=alphas_enetcvi, l1_ratio=l1s_enetcvi, cv=cvreg_inner, max_iter=10000, normalize=False, n_jobs=4)\n","enet_cvi_score = []\n","\n","for k, (trainCV, testCV) in enumerate(cvreg_outer.split(X3, y3, groups=groups3)):\n","    enet_cvi.fit(X3.values[trainCV], y3[trainCV])\n","    print(\"[fold {0}] alpha: {1:.5f}, l1_ratio: {2:.5f}, score: {3:.5f}\".\n","          format(k, enet_cvi.alpha_, enet_cvi.l1_ratio_, enet_cvi.score(X3.values[testCV], y3[testCV])))\n","    enet_cvi_score.append(enet_cvi.score(X3.values[testCV], y3[testCV]))\n","\n","print('ElasticNetCVI')\n","print('Mean: ', np.mean(enet_cvi_score))\n","print('SD: ', np.std(enet_cvi_score))\n","\n","feature_list_eni = []\n","for i in list(range(0,len(X3.columns))):\n","    feature_list_eni.append([enet_cvi.coef_[i], X3.columns[i]])\n","    df_featimp_eni = pd.DataFrame(data=feature_list_eni, columns=['coef','feature'])\n","df_featimp_eni.reindex(df_featimp_eni.coef.abs().sort_values(ascending=False).index)\n","df_featimp_sum_eni = df_featimp_eni[(df_featimp_eni['coef'] > .001) | (df_featimp_eni['coef'] < -.001)]\n","print(df_featimp_sum_eni.reindex(df_featimp_sum_eni.coef.abs().sort_values(ascending=False).index))\n","\n","##########################################\n","### Regularization Performance Summary ###\n","##########################################\n","\n","print(\"RidgeCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(ridge_cvb_score), np.std(ridge_cvb_score), ridge_cvb.alpha_))\n","print(\"RidgeCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(ridge_cvi_score), np.std(ridge_cvi_score), ridge_cvi.alpha_))\n","print(\"LassoCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(lasso_cvb_score), np.std(lasso_cvb_score), lasso_cvb.alpha_))\n","print(\"LassoCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(lasso_cvi_score), np.std(lasso_cvi_score), lasso_cvi.alpha_))\n","print(\"LLarsCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(llars_cvb_score), np.std(llars_cvb_score), llars_cvb.alpha_))\n","print(\"LLarsCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}\".\n","      format(np.mean(llars_cvi_score), np.std(llars_cvi_score), llars_cvi.alpha_))\n","print(\"E-NetCVB Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; l1_ratio: {3:.5f}\".\n","      format(np.mean(enet_cvb_score), np.std(enet_cvb_score), enet_cvb.alpha_, enet_cvb.l1_ratio_))\n","print(\"E-NetCVI Mean: {0:.5f}; SD: {1:.5f}; alpha: {2:.5f}; l1_ratio: {3:.5f}\".\n","      format(np.mean(enet_cvi_score), np.std(enet_cvi_score), enet_cvi.alpha_, enet_cvi.l1_ratio_))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}